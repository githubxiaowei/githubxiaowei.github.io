<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>RNN 训练算法 —— BPDC (Backpropagation-Decorrelation)</title>
    <url>/2021/03/BPDC/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>考虑模型循环网络模型：</p>
<script type="math/tex; mode=display">
x(k+1) = (1-\Delta t)x(k) +  \Delta t Wf[x(k)] \tag1{}</script><p>其中 $x(k) \in R^N$表示网络节点在激活前的状态，$W\in R^{N\times N}$表示网络结点之间相互连接的权重，网络的输出节点为 $\{x_i(k)| i\in O\}$，$O$为所有输出（或称“观测”）单元的下标集合</p>
<p>训练的目标是为了减少观测状态和预期值之间误差，即最小化损失函数：</p>
<script type="math/tex; mode=display">
E = \frac{1}{2}\sum_{k=1}^K \sum_{i\in O} [x_i(k) - d_i(k)]^2 \tag{2}</script><p>其中 $d_i(k)$ 表示 $k$ 时刻第 $i$ 个节点的预期值</p>
<span id="more"></span>
<p><img src="RNN.png" alt="RNN"></p>
<h1 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h1><script type="math/tex; mode=display">
W \equiv
\begin{bmatrix}
\text{-----}  w_1^T \text{-----} \\
\vdots \\
\text{-----}  w_N^T \text{-----} 
\end{bmatrix}_{N\times N}</script><p>将矩阵 $W$ 拉成列向量，记为 $w$</p>
<script type="math/tex; mode=display">
w = [w_1^T, \cdots, w_N^T]^T \in R^{N^2}</script><p>把所有时间的状态拼成列向量，记为 $x$</p>
<script type="math/tex; mode=display">
x = [x^T(1), \cdots, x^T(K)]^T \in R^{NK}</script><p>将RNN 的训练视为约束优化问题，(1)式转化成约束条件:</p>
<script type="math/tex; mode=display">
g(k+1) \equiv  -x(k+1) + (1-\Delta t)x(k) +  \Delta t Wf[x(k)] , \quad k=1,\ldots ,K \tag{3}</script><p>记</p>
<script type="math/tex; mode=display">
g = [g^T(1), \ldots, g^T(K)]^T \in R^{NK}</script><h1 id="Atiya-Parlos-算法回顾"><a href="#Atiya-Parlos-算法回顾" class="headerlink" title="Atiya-Parlos 算法回顾"></a>Atiya-Parlos 算法回顾</h1><p>以上是经典的梯度下降法的思维，但是 Atiya-Parlos 提出了另一种优化思路：不是朝着参数的梯度方向更新，但仍使代价函数下降</p>
<p>该算法的思想是互换网络状态 $x(k)$ 和权重矩阵 $W$ 的作用：将状态视为控制变量，并根据 $x(k)$ 的变化确定权重的变化。 换句话说，我们计算 $E$ 相对于状态 $x(k)$ 的梯度，并假设状态在该梯度的负方向 $\displaystyle{\Delta x_i(k) = -\eta\frac{\partial E}{\partial x_i(k)} }$ 上有微小变化。</p>
<p>接下来，我们确定权重 $W$ 的变化 $\Delta w$，以使由权重变化导致的状态变化尽可能地接近目标变化 $\Delta x$</p>
<p>该算法的细节如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta x &= -\eta \left(\frac{\partial E}{\partial x_i} \right)^T \\
&= -\eta e^T\\
&= -\eta [e(1), \ldots, e(K)]^T \\\\
e_i(k)&= \begin{cases}
   x_i(k) - d_i(k), &\text{if } i\in O, \\
   0, &\text{otherwise. } 
\end{cases} 
k \in 1,\ldots,K.
\end{aligned}</script><p>由约束条件得：</p>
<script type="math/tex; mode=display">
\frac{\partial g}{\partial x} \Delta x = - \frac{\partial g}{\partial w} \Delta w</script><p>故已知 $\Delta x$ 时，可得：</p>
<script type="math/tex; mode=display">
\Delta w = -\left[\left(\frac{\partial g}{\partial w}\right)^T \left(\frac{\partial g}{\partial w}\right)\right]^{-1} \left(\frac{\partial g}{\partial w}\right)^T\left( \frac{\partial g}{\partial x}\right) \Delta x</script><p>需要注意逆矩阵不一定存在，故</p>
<script type="math/tex; mode=display">
\Delta w = -\left[\left(\frac{\partial g}{\partial w}\right)^T \left(\frac{\partial g}{\partial w}\right) + \epsilon I \right]^{-1} \left(\frac{\partial g}{\partial w}\right)^T\left( \frac{\partial g}{\partial x}\right) \Delta x</script><p>这就是权重 $W$ 的更新规则</p>
<h1 id="计算细节"><a href="#计算细节" class="headerlink" title="计算细节"></a>计算细节</h1><p>计算 $\frac{\partial g}{\partial w}$</p>
<script type="math/tex; mode=display">
\frac{\partial g}{\partial w} =
 \begin{bmatrix}
\frac{\partial g(1)}{\partial w}\\
\vdots \\
\frac{\partial g(K)}{\partial w}
\end{bmatrix}
=  \Delta t \begin{bmatrix}
\frac{\partial  Wf[x(0)] }{\partial w}\\
\vdots \\
\frac{\partial Wf[x(K-1)] }{\partial w}
\end{bmatrix}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial Wf[x(k)]}{\partial w}
&= \begin{bmatrix}
\frac{\partial w_1^Tf[x(k)]}{\partial w}\\
\vdots \\
\frac{\partial w_N^Tf[x(k)]}{\partial w}
\end{bmatrix} \color{red}{记 f_k = [f(x_1), \ldots, f(x_N(k))]^T}\\\\
& = \begin{bmatrix}
f_k^T &&& \\
 & f_k^T&& \\
 && \ddots & \\
 &&& f_k^T
\end{bmatrix}_{N\times N^2} \\\\
&\triangleq  F(k)
\end{aligned}</script><p>故</p>
<script type="math/tex; mode=display">
\frac{\partial g}{\partial w} = 
\Delta t \begin{bmatrix}
F(0)\\
\vdots \\
F(K-1)
\end{bmatrix}_{NK \times N^2}</script><script type="math/tex; mode=display">
\begin{aligned}
&\frac{1}{\Delta t^2}\left(\frac{\partial g}{\partial w}\right)^T \left(\frac{\partial g}{\partial w}\right)  \\
&= 
 \begin{bmatrix}
F^T(0) &
\cdots &
F^T(K-1)
\end{bmatrix}
 \begin{bmatrix}
F(0)\\
\vdots \\
F(K-1)
\end{bmatrix} \\\\
&= 
\sum_{k=0}^{K-1} F^T(k)F(k) \\\\
&=\begin{bmatrix}
 \sum_{k=0}^{K-1} f_k f_k^T &&& \\
 &  \sum_{k=0}^{K-1} f_k f_k^T  && \\
 && \ddots & \\
 &&&  \sum_{k=0}^{K-1} f_k f_k^T 
\end{bmatrix}_{N^2 \times N^2} \\\\
&\triangleq diag\{C_{K-1}\}
\end{aligned}</script><p>令</p>
<script type="math/tex; mode=display">
\gamma = 
\begin{bmatrix}
\gamma(1)\\ 
\gamma(2) \\ 
\vdots \\
 \gamma(K)
\end{bmatrix}_{NK}
= \frac{\partial g}{\partial x} \Delta x</script><p>$\gamma$ 表示由 $\Delta x$ 提供的误差信息，它的计算放在本文最后，先假设它已经求出来了</p>
<p>则</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \left(\frac{\partial g}{\partial w}\right)^T\left( \frac{\partial g}{\partial x}\right) \Delta x  \\
&= 
\Delta t \begin{bmatrix}
F^T(0) &
\cdots &
F^T(K-1)
\end{bmatrix}_{N^2 \times NK}
\begin{bmatrix}
\gamma(1)\\ 
\gamma(2) \\ 
\vdots \\
 \gamma(K)
\end{bmatrix}_{NK} \\
&= \Delta t\sum_{k=1}^K F^T(k-1)\gamma(k) \\
&=\Delta t \sum_{k=1}^K   \begin{bmatrix}
f_{k-1} &&& \\
 & f_{k-1}&& \\
 && \ddots & \\
 &&& f_{k-1}
\end{bmatrix}_{N^2 \times N}
\begin{bmatrix}
\gamma_1(k)\\ 
\gamma_2(k) \\ 
\vdots \\
 \gamma_N(k)
\end{bmatrix}_{N}\\\\
&=\Delta t \begin{bmatrix}
 \sum_{k=1}^K f_{k-1} \gamma_1(k)\\ 
 \sum_{k=1}^K f_{k-1} \gamma_2(k) \\ 
\vdots \\
 \sum_{k=1}^K f_{k-1} \gamma_N(k)
\end{bmatrix}_{N^2}\\\\
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta w 
&=  -\left[\left(\frac{\partial g}{\partial w}\right)^T \left(\frac{\partial g}{\partial w}\right) + \epsilon I\right]^{-1} \left(\frac{\partial g}{\partial w}\right)^T\left( \frac{\partial g}{\partial x}\right) \Delta x \\
&= - \frac{1}{\Delta t} \begin{bmatrix}
C_{K-1}^{-1} \sum_{k=1}^K f_{k-1} \gamma_1(k)\\ 
C_{K-1}^{-1} \sum_{k=1}^K f_{k-1} \gamma_2(k) \\ 
\vdots \\
C_{K-1}^{-1} \sum_{k=1}^K f_{k-1} \gamma_N(k)
\end{bmatrix}_{N^2}\\\\
\Delta W 
&= - \frac{1}{\Delta t} \begin{bmatrix}
 \sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \gamma_1(k)\\ 
 \sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \gamma_2(k) \\ 
\vdots \\
 \sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \gamma_N(k)
\end{bmatrix}_{N\times N} \\
&= - \frac{1}{\Delta t}  \sum_{k=1}^K\begin{bmatrix}
 f_{k-1}^T \gamma_1(k)\\ 
 f_{k-1}^T \gamma_2(k) \\ 
\vdots \\
f_{k-1}^T \gamma_N(k)
\end{bmatrix}_{N\times N} C_{K-1}^{-1} \\ 
\end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">
C_{K-1} = \epsilon I + \sum_{r=0}^{K-1} f_r f_r^T</script><p>注意：上述 $\Delta W$ 是基于 $1,2,\ldots, K$ 整个时间段的更新，不妨称之为 $\Delta W_{batch}$</p>
<p>下面将更新公式拆解在线更新（online updating）的形式：</p>
<script type="math/tex; mode=display">
\Delta W^{batch}(K)= \Delta W(1) + \cdots + \Delta W(K)</script><p>等式右端对应每一时刻的更新量</p>
<p>在第 $K$ 时刻的第 $i$ 个神经元的输入权重的更新量：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta w^T_{i}(K) &=  - \frac{1}{\Delta t} \sum_{k=1}^{K} f_{k-1}^TC_{K-1}^{-1}  \gamma_i(k) + \frac{1}{\Delta t} \sum_{k=1}^{K-1} f_{k-1}^TC_{K-2}^{-1}  \gamma_i(k)\\\\
&= - \frac{1}{\Delta t}   f_{K-1}^TC_{K-1}^{-1} \gamma_i(K)  - \frac{1}{\Delta t} \sum_{k=1}^{K-1} f_{k-1}^T (C_{K-1}^{-1} - C_{K-2}^{-1}) \gamma_i(k) \\\\
&=- \frac{1}{\Delta t}   f_{K-1}^T C_{K-1}^{-1}\gamma_i(K)  - \frac{1}{\Delta t} \sum_{k=1}^{K-1} f_{k-1}^T C_{K-2}^{-1}\gamma_i(k)(C_{K-2}C_{K-1}^{-1} - I) \\\\
&= - \frac{1}{\Delta t}   f_{K-1}^TC_{K-1}^{-1} \gamma_i(K)  -   \Delta w_i^{batch}(K-1)(C_{K-2}C_{K-1}^{-1}- I) \\\\
&= - \frac{1}{\Delta t}   f_{K-1}^TC_{K-1}^{-1} \gamma_i(K)  -  \sum_{k=1}^{K-1} \Delta w^T_i(k)  (C_{K-2}C_{K-1}^{-1}- I)
\end{aligned}</script><p>可以看出，APRL 的更新规则由当前时刻的误差和 w 的累计更新（动量）组成</p>
<p>随着 $K \to \infty$，易知$\sum_{k=1}^{K-1} \Delta w^T_i(k) \to const, C_{K-2}C_{K-1}^{-1} \to I$，所以第二项趋于零</p>
<h1 id="BPDC-更新规则"><a href="#BPDC-更新规则" class="headerlink" title="BPDC 更新规则"></a>BPDC 更新规则</h1><p>BPDC 对 APRL 的在线算法做了简单粗暴的近似</p>
<p>该近似不试图累积完整的相关矩阵 $C_k$，也舍弃了先前误差的累积，而且只计算瞬时相关 $C(k)$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta w^T_{i}(k+1) &= - \frac{1}{\Delta t}  f_{k}^TC(k)^{-1}  \gamma_i(k+1)  \\\\
C(k) &= \epsilon I + f_k f_k^T
\end{aligned}</script><p>利用<a href="https://blog.csdn.net/itnerd/article/details/105612704">矩阵求逆引理</a>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
C(k)^{-1} &= (\epsilon I + f_k f_k^T)^{-1} \\
&= \frac{1}{\epsilon}I - \frac{1}{\epsilon} \frac{ff^T}{\epsilon + f^Tf}
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta w^T_{i}(k+1) &= - \frac{1}{\Delta t}  f_{k}^T\left( \frac{1}{\epsilon}I - \frac{1}{\epsilon} \frac{ff^T}{\epsilon + f^Tf}\right)  \gamma_i(k+1)  \\\\
&=  - \frac{1}{\Delta t}   \frac{f^T}{\epsilon + f^Tf}  \gamma_i(k+1)  
\end{aligned}</script><h1 id="计算-gamma"><a href="#计算-gamma" class="headerlink" title="计算 $\gamma$"></a>计算 $\gamma$</h1><script type="math/tex; mode=display">
\gamma = 
\begin{bmatrix}
\gamma(1)\\ 
\gamma(2) \\ 
\vdots \\
 \gamma(K)
\end{bmatrix}_{NK}
= \frac{\partial g}{\partial x} \Delta x = -\eta \frac{\partial g}{\partial x} [e(1), \ldots, e(K)]^T</script><p>关键在与计算 $\frac{\partial g}{\partial x}$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial g}{\partial x} &= 
 \begin{bmatrix}
\frac{\partial g_1}{\partial x(1)} & \ldots & \frac{ \partial g_1}{\partial x(K)}\\
\vdots & \ddots & \vdots\\
\frac{\partial g_K}{\partial x(1)} & \ldots & \frac{ \partial g_K}{\partial x(K)}
\end{bmatrix}\\\\
&=  
\begin{bmatrix}
\frac{\partial g_1}{\partial x(1)} & 0&  0 &\ldots & 0\\
\frac{\partial g_2}{\partial x(1)} & \frac{\partial g_2}{\partial x(2)}& 0 &\ldots & 0  \\
0 & \frac{\partial g_3}{\partial x(2)} & \frac{\partial g_3}{\partial x(3)} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \frac{\partial g_K}{\partial x(K-1)}& \frac{\partial g_K}{\partial x(K)}
\end{bmatrix} \\\\
&=  
\begin{bmatrix}
-I & 0&  0 &\ldots & 0\\
(1-\Delta t )I + \Delta t W D(1) & -I& 0 &\ldots & 0  \\
0 & (1-\Delta t )I + \Delta t W D(2) & -I & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 &(1-\Delta t )I + \Delta t W D(K-1)& -I
\end{bmatrix}
\end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">
D(k) =  \begin{bmatrix}
 f'(x_1(k)) & \cdots&0\\
\vdots & \ddots & \vdots\\
0& \ldots &  f'(x_N(k))
\end{bmatrix}_{N \times N}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned}
\gamma &= -\eta \frac{\partial g}{\partial x} [e(1), \ldots, e(K)]^T\\
&= -\eta\begin{bmatrix}
-I & 0&  0 &\ldots & 0\\
(1-\Delta t )I + \Delta t WD(1) & -I& 0 &\ldots & 0  \\
0 & (1-\Delta t )I + \Delta t W D(2) & -I & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 &(1-\Delta t )I + \Delta t W D(K-1)& -I
\end{bmatrix}
\begin{bmatrix}
e(1) \\
e(2)  \\
e(3) \\
\vdots  \\
e(K)
\end{bmatrix} \\\\
&= 
-\eta \begin{bmatrix}
-e(1) \\
[(1-\Delta t )I + \Delta t W D(1)]e(1) - e(2)  \\
[(1-\Delta t )I + \Delta t W D(2)]e(2) - e(3)  \\
\vdots  \\
[(1-\Delta t )I + \Delta t W D(K-1)]e(K-1) - e(K) 
\end{bmatrix}_{NK \times 1}
\end{aligned}</script><p>代入到 BPDC 更新规则：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta w^T_{i}(k+1) 
&=  - \frac{1}{\Delta t}   \frac{f^T}{\epsilon + f^Tf}  \color{red}{ \gamma_i(k+1)}  \\
&= \frac{\color{red}{\eta}}{\Delta t}   \frac{f^T}{\epsilon + f^Tf} \color{red}\{ (1-\Delta t )e_i(k) + \Delta t \sum_{s\in O}w_{is} f'(x_s(k))e_s(k) - e_i(k+1) \} 
\end{aligned}</script><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li>J.J. Steil, Backpropagation-decorrelation: online recurrent learning with O(N) complexity, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN), vol. 1, 2004, pp. 843–848.</li>
<li>J.J. Steil, Online stability of backpropagation-decorrelation recurrent learning, Neurocomputing 69 (2006) 642–650.</li>
<li>J.J. Steil, Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning, Neural Networks 20 (3) (2007) 353–364.</li>
</ul>
]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>BPDC</tag>
        <tag>训练</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>无标度网络的生成模型</title>
    <url>/2021/02/%E6%97%A0%E6%A0%87%E5%BA%A6%E7%BD%91%E7%BB%9C%E7%9A%84%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>1999 年 Barabási 和 Albert 提出了无标度网络模型（简称 BA 模型）。无标度网络的重要特征为： 无标度网络的节点度分布服从幂律分布。</p>
<p>无标度网络的度分布 $p(d)$ 满足<script type="math/tex">p(d)\sim d^{-\alpha}，</script>其中 $d$ 代表度的大小， $\alpha$ 为度分布的幂律指数。 真实网络 $\alpha$ 值一般介于 2~3之间。</p>
<p>近年来越来越多的研究表明， 真实世界网络既不是规则网络， 也不是随机网络， 而是兼具小世界和无标度特性的复杂网络， 具有与规则网络和随机网络截然不同的统计特性。</p>
<span id="more"></span>
<p>本文采用的无标度网络生成模型是由 Barabási 和 Albert 于 1999 年提出的增长网络网络模型（BA 模型）。在该模型中，网络初始时具有 $m_0$ 个节点，两两互连。 之后每过一个时间单位增加一个新节点。新节点从当前网络中选择$m(m ≤ m_0)$ 个节点与之连接， 某节点 $v_i$ 被选中的概率 $p(v_i)$ 与其节点度 $d_i$ 的大小成正比，即<script type="math/tex">p(v_i) = \frac{d_i}{\sum_j d_j}</script>经过 t 个时间单位后，网络中含有 $m_0+t$ 个节点，$m_0(m_0-1)/2+mt$条边。可以证明当 t 足够大时， 按此规律增长的网络的度分布为幂指数等于 3 的幂律分布。</p>
<p>依据新节点的连接规律，建立节点度演化的动力学模型：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial d_i}{\partial t} &= m \frac{\partial d_i}{\sum_j d_j} \\
&= m \frac{d_i}{2\left( \frac{m_0(m_0-1)}{2} + mt\right)} \\
&= m \frac{d_i}{\left( m_0(m_0-1) + 2mt\right)} \\
&\simeq \frac{d_i}{2t}
\end{aligned}</script><p>其中最后一个等式在 $t$ 足够大时近似成立。 将节点 $i$ 加入网络的时间记为 $t_i$，<br>则有初始条件 $d_i(t_i) = m$。解得</p>
<script type="math/tex; mode=display">
d_i = \left\{ 
\begin{array}{ll}
0, & t < t_i, \\
m(\frac{t}{t_i})^{0.5}, &t\geq t_i.
\end{array}
\right.</script><p>在 $t$ 足够大， 对任意节点 $i$， 其度的大小满足</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(d_i(t) < d) &= P\left( t_i > \frac{m^2 t}{d^2}\right) \\
&= 1 - P\left(t_i \leq \frac{m^2 t}{d^2}\right) \\
&= 1 - \frac{m^2 t}{d^2(m_0 + t)} \\
&\simeq 1- \frac{m^2}{d^2}
\end{aligned}</script><p>其中第三个等式成立的原因是加入节点的时间是等间隔的。上式正是网络节点度的概率分布函数， 可以求出节点度的概率密度函数 $p(d)$ 为</p>
<script type="math/tex; mode=display">
p(d) = \frac{\partial P(d_i(t) < d)}{\partial d} = 2\frac{m^2}{d^3}</script><p>可知所生成网络的幂律分布的指数为 3。</p>
<p>下面的matlab程序模拟了BA网络的演化过程：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">scale_free</span><span class="params">(N,m0,m)</span></span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%param  N: num of vertices 期望节点数</span></span><br><span class="line"><span class="comment">%param m0: num of initial vertices 初始边数</span></span><br><span class="line"><span class="comment">%param  m: num of vertices a new node try to connect 新节点连接的边数</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line">tic;</span><br><span class="line"></span><br><span class="line">I = <span class="number">2</span> ;    <span class="comment">%生成的网络个数，只为统计需要</span></span><br><span class="line"></span><br><span class="line">realization_of_distribution = sparse( I , N ) ;</span><br><span class="line"><span class="keyword">for</span> J = <span class="number">1</span> : I</span><br><span class="line">    format long;</span><br><span class="line"></span><br><span class="line"> 	<span class="comment">%初始化邻接矩阵，前m0个节点两两互连</span></span><br><span class="line">    adjacent_matrix = sparse( m0 , m0 ) ;</span><br><span class="line">    <span class="keyword">parfor</span> <span class="built_in">i</span> = <span class="number">1</span> : m0</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : m0</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">j</span> ~= <span class="built_in">i</span></span><br><span class="line">                adjacent_matrix( <span class="built_in">i</span> , <span class="built_in">j</span> ) = <span class="number">1</span> ;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    adjacent_matrix = sparse( adjacent_matrix ) ;</span><br><span class="line"></span><br><span class="line">	<span class="comment">% 计算当前节点度分布</span></span><br><span class="line">    node_degree = sparse( <span class="number">1</span> , m0 ) ;</span><br><span class="line">    <span class="keyword">for</span> p = <span class="number">1</span> : m0</span><br><span class="line">        node_degree( p ) = sum( adjacent_matrix( <span class="number">1</span> : m0 , p ) ) ;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">% 开始演化</span></span><br><span class="line">    <span class="keyword">for</span> iteration = m0 + <span class="number">1</span> : N</span><br><span class="line">        total_degree = <span class="number">2</span> * m * ( iteration - m0 <span class="number">-1</span> ) + m0*(m0<span class="number">-1</span>) ; <span class="comment">% m*2</span></span><br><span class="line">        degree_frequency = node_degree / total_degree ;</span><br><span class="line">        cum_distribution = cumsum( degree_frequency ) ;</span><br><span class="line"></span><br><span class="line">        choose = <span class="built_in">zeros</span>( <span class="number">1</span> , m ) ;</span><br><span class="line">        <span class="keyword">for</span> new_edge = <span class="number">1</span>:m</span><br><span class="line">            r = <span class="built_in">rand</span>(<span class="number">1</span>) ;</span><br><span class="line">            choose_edge = <span class="built_in">find</span>( cum_distribution &gt;= r ,<span class="number">1</span>) ;</span><br><span class="line">            <span class="keyword">while</span> any(choose == choose_edge)</span><br><span class="line">                r = <span class="built_in">rand</span>(<span class="number">1</span>) ;</span><br><span class="line">                choose_edge = <span class="built_in">find</span>(  cum_distribution &gt;= r,<span class="number">1</span>) ;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            choose(new_edge) = choose_edge;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k = <span class="number">1</span> : m</span><br><span class="line">            adjacent_matrix( iteration , choose(k) ) = <span class="number">1</span> ;</span><br><span class="line">            adjacent_matrix( choose(k) , iteration ) = <span class="number">1</span> ;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p = <span class="number">1</span> : iteration</span><br><span class="line">            node_degree(p) = sum( adjacent_matrix( <span class="number">1</span> : iteration , p ) ) ;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    number_of_nodes_with_equal_degree = <span class="built_in">zeros</span>( <span class="number">1</span> , N ) ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">parfor</span> <span class="built_in">i</span> = <span class="number">1</span> : N</span><br><span class="line">        number_of_nodes_with_equal_degree(<span class="built_in">i</span>) = <span class="built_in">length</span>( <span class="built_in">find</span>( node_degree == <span class="built_in">i</span> ) ) ;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    realization_of_distribution( J , : ) = number_of_nodes_with_equal_degree ;</span><br><span class="line"></span><br><span class="line">    save([<span class="string">&#x27;adj_&#x27;</span>,num2str(J)],<span class="string">&#x27;adjacent_matrix&#x27;</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%&#123;</span></span><br><span class="line"><span class="comment">%plot degree distribution 在双对数坐标下画图</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">average = sum( realization_of_distribution )/ ( I * N );</span></span><br><span class="line"><span class="comment">loglog( 1:N , average , &#x27;*&#x27; )</span></span><br><span class="line"><span class="comment">axis([1 N 0.0000001 0.9])</span></span><br><span class="line"><span class="comment">hold on;</span></span><br><span class="line"><span class="comment">x = 1:N;</span></span><br><span class="line"><span class="comment">y = 2 * m^2 * x .^ ( -3 ) ;</span></span><br><span class="line"><span class="comment">loglog( x , y , &#x27;r&#x27; ) ;  %  p(k)=2*m^2*k^(-3)</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">%&#125;</span></span><br><span class="line">toc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>人工生成网络的概率质量函数（网络节点数 $N$ 分别为 50、 100、 200、 400）</p>
<p><img src="degree_dist.png" alt="生成网络的节点度分布"></p>
<p>图中直线为理论结果： $p(d)=2\frac{m^2}{d^3}$。</p>
<p><img src="/images/世界人民大团结万岁.gif" alt=""></p>
]]></content>
      <categories>
        <category>复杂网络</category>
      </categories>
      <tags>
        <tag>无标度</tag>
        <tag>网络</tag>
      </tags>
  </entry>
</search>
