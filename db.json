{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo-16x16.png","path":"images/logo-16x16.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo-32x32.png","path":"images/logo-32x32.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"source/images/alen.png","path":"images/alen.png","modified":0,"renderable":0},{"_id":"source/images/bg.jpeg","path":"images/bg.jpeg","modified":0,"renderable":0},{"_id":"source/images/logo-header.png","path":"images/logo-header.png","modified":0,"renderable":0},{"_id":"source/images/世界人民大团结万岁.gif","path":"images/世界人民大团结万岁.gif","modified":0,"renderable":0},{"_id":"source/images/全世界无产者联合起来.gif","path":"images/全世界无产者联合起来.gif","modified":0,"renderable":0}],"Cache":[{"_id":"source/_data/styles.styl","hash":"72378bab80c7d5ff2821c57e5b3fd6f73de679ee","modified":1614699340874},{"_id":"source/_posts/构造极限环.md","hash":"6e20d88fbe6d271966ce3cb94022a08682e7f808","modified":1616511588664},{"_id":"source/_posts/计算一阶导数的四阶中心差分格式.md","hash":"64d7999ec3bbb4ba128d073a3590e5aeacbac496","modified":1616511908431},{"_id":"source/_posts/BPDC.md","hash":"2b9fced550aa14f527150ad4e16b27765c7d3380","modified":1616499891482},{"_id":"source/_posts/无标度网络的生成模型.md","hash":"9311641438e5606be4d821ffcf76eedfab94f0ed","modified":1614693456748},{"_id":"source/about/index.md","hash":"ebdf4e0396999afc6d63a827ab39af3af669e0b2","modified":1614155928533},{"_id":"source/images/logo-header.png","hash":"651a9b5148d097e7498b27bb2e502ea10e0041b4","modified":1614611670770},{"_id":"source/categories/index.md","hash":"3403bb5c758a63c08848adffbed26f90e63d6928","modified":1614673078270},{"_id":"source/_posts/BPDC/RNN.png","hash":"b3771ba252ccf1c2a1553f141e49110341ac7305","modified":1616499373863},{"_id":"source/images/世界人民大团结万岁.gif","hash":"7cf242aea54914d5d12d0046e6b8eecc80eaac59","modified":1614693116433},{"_id":"source/_posts/构造极限环/pic1.png","hash":"c12b5e363eed8e5f40e8b0c65af7edf15ac526c2","modified":1616511404459},{"_id":"source/_posts/构造极限环/pic2.png","hash":"aa7187703b6507cd7ed895f14c62a6ae5428d1d4","modified":1616511405791},{"_id":"source/_posts/计算一阶导数的四阶中心差分格式/四阶中心差分.png","hash":"5164571d132ab26494d5986f465193d6c91ac8d7","modified":1616511801632},{"_id":"source/images/alen.png","hash":"6898877141e21218a781fd932112defe5ea23e05","modified":1614701523801},{"_id":"source/tags/index.md","hash":"e43ebbcec122ce235dbd9f473de7b19ed8a707d3","modified":1614673155966},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1614670343621},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1614670343621},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1614670343621},{"_id":"themes/next/.gitignore","hash":"56f3470755c20311ddd30d421b377697a6e5e68b","modified":1614670343625},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1614670343625},{"_id":"themes/next/_config.yml","hash":"b9468da77ee78d8ac4ba419831b0c02fb6f84411","modified":1616510959008},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1614670343625},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1614670343625},{"_id":"themes/next/package.json","hash":"62fad6de02adbbba9fb096cbe2dcc15fe25f2435","modified":1614670343629},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1614670343625},{"_id":"themes/next/README.md","hash":"9b4b7d66aca47f9c65d6321b14eef48d95c4dff1","modified":1614670343625},{"_id":"themes/next/gulpfile.js","hash":"1b4fc262b89948937b9e3794de812a7c1f2f3592","modified":1614670343625},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"aa4cb7aff595ca628cb58160ee1eee117989ec4e","modified":1614670343621},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"e554931b98f251fd49ff1d2443006d9ea2c20461","modified":1614670343621},{"_id":"themes/next/.github/config.yml","hash":"1d3f4e8794986817c0fead095c74f756d45f91ed","modified":1614670343621},{"_id":"themes/next/.github/issue-close-app.yml","hash":"7cba457eec47dbfcfd4086acd1c69eaafca2f0cd","modified":1614670343625},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1a435c20ae8fa183d49bbf96ac956f7c6c25c8af","modified":1614670343621},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1614670343625},{"_id":"themes/next/.github/lock.yml","hash":"61173b9522ebac13db2c544e138808295624f7fd","modified":1614670343625},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1614670343625},{"_id":"themes/next/.github/release-drafter.yml","hash":"3cc10ce75ecc03a5ce86b00363e2a17eb65d15ea","modified":1614670343625},{"_id":"themes/next/.github/stale.yml","hash":"fdf82de9284f8bc8e0b0712b4cc1cb081a94de59","modified":1614670343625},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1614670343625},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1614670343625},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"c7a994b9542040317d8f99affa1405c143a94a38","modified":1614670343625},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1614670343625},{"_id":"themes/next/docs/DATA-FILES.md","hash":"cddbdc91ee9e65c37a50bec12194f93d36161616","modified":1614670343625},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1614670343625},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"94dc3404ccb0e5f663af2aa883c1af1d6eae553d","modified":1614670343625},{"_id":"themes/next/languages/ar.yml","hash":"9815e84e53d750c8bcbd9193c2d44d8d910e3444","modified":1614670343625},{"_id":"themes/next/languages/de.yml","hash":"74c59f2744217003b717b59d96e275b54635abf5","modified":1614670343625},{"_id":"themes/next/languages/default.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1614670343625},{"_id":"themes/next/languages/en.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1614670343625},{"_id":"themes/next/languages/es.yml","hash":"c64cf05f356096f1464b4b1439da3c6c9b941062","modified":1614670343625},{"_id":"themes/next/languages/fa.yml","hash":"3676b32fda37e122f3c1a655085a1868fb6ad66b","modified":1614670343625},{"_id":"themes/next/languages/fr.yml","hash":"752bf309f46a2cd43890b82300b342d7218d625f","modified":1614670343625},{"_id":"themes/next/languages/hu.yml","hash":"b1ebb77a5fd101195b79f94de293bcf9001d996f","modified":1614670343625},{"_id":"themes/next/languages/id.yml","hash":"572ed855d47aafe26f58c73b1394530754881ec2","modified":1614670343625},{"_id":"themes/next/languages/it.yml","hash":"44759f779ce9c260b895532de1d209ad4bd144bf","modified":1614670343625},{"_id":"themes/next/languages/ja.yml","hash":"0cf0baa663d530f22ff380a051881216d6adcdd8","modified":1614670343625},{"_id":"themes/next/languages/ko.yml","hash":"0feea9e43cd399f3610b94d755a39fff1d371e97","modified":1614670343625},{"_id":"themes/next/languages/nl.yml","hash":"5af3473d9f22897204afabc08bb984b247493330","modified":1614670343625},{"_id":"themes/next/languages/pt-BR.yml","hash":"67555b1ba31a0242b12fc6ce3add28531160e35b","modified":1614670343625},{"_id":"themes/next/languages/pt.yml","hash":"718d131f42f214842337776e1eaddd1e9a584054","modified":1614670343625},{"_id":"themes/next/languages/ru.yml","hash":"e993d5ca072f7f6887e30fc0c19b4da791ca7a88","modified":1614670343625},{"_id":"themes/next/languages/tr.yml","hash":"fe793f4c2608e3f85f0b872fd0ac1fb93e6155e2","modified":1614670343625},{"_id":"themes/next/languages/uk.yml","hash":"3a6d635b1035423b22fc86d9455dba9003724de9","modified":1614670343625},{"_id":"themes/next/languages/vi.yml","hash":"93393b01df148dcbf0863f6eee8e404e2d94ef9e","modified":1614670343625},{"_id":"themes/next/languages/zh-CN.yml","hash":"a1f15571ee7e1e84e3cc0985c3ec4ba1a113f6f8","modified":1614670343625},{"_id":"themes/next/languages/zh-HK.yml","hash":"3789f94010f948e9f23e21235ef422a191753c65","modified":1614670343625},{"_id":"themes/next/languages/zh-TW.yml","hash":"8c09da7c4ec3fca2c6ee897b2eea260596a2baa1","modified":1614670343625},{"_id":"themes/next/layout/_layout.swig","hash":"6a6e92a4664cdb981890a27ac11fd057f44de1d5","modified":1614670343625},{"_id":"themes/next/layout/archive.swig","hash":"e4e31317a8df68f23156cfc49e9b1aa9a12ad2ed","modified":1614670343629},{"_id":"themes/next/layout/category.swig","hash":"1bde61cf4d2d171647311a0ac2c5c7933f6a53b0","modified":1614670343629},{"_id":"themes/next/layout/index.swig","hash":"7f403a18a68e6d662ae3e154b2c1d3bbe0801a23","modified":1614670343629},{"_id":"themes/next/layout/page.swig","hash":"db581bdeac5c75fabb0f17d7c5e746e47f2a9168","modified":1614670343629},{"_id":"themes/next/layout/post.swig","hash":"2f6d992ced7e067521fdce05ffe4fd75481f41c5","modified":1614670343629},{"_id":"themes/next/layout/tag.swig","hash":"0dfb653bd5de980426d55a0606d1ab122bd8c017","modified":1614670343629},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1614670343629},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1614670343625},{"_id":"themes/next/docs/MATH.md","hash":"d645b025ec7fb9fbf799b9bb76af33b9f5b9ed93","modified":1614670343625},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"8b6e4b2c9cfcb969833092bdeaed78534082e3e6","modified":1614670343625},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"12d99fb8b62bd9e34d9672f306c9ae4ace7e053e","modified":1614670343621},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"c3e6b8196c983c40fd140bdeca012d03e6e86967","modified":1614670343621},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d3efc0df0275c98440e69476f733097916a2d579","modified":1614670343621},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"53df7d537e26aaf062d70d86835c5fd8f81412f3","modified":1614670343621},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"0bd2d696f62a997a11a7d84fec0130122234174e","modified":1614670343625},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1614670343625},{"_id":"themes/next/docs/ru/README.md","hash":"85dd68ed1250897a8e4a444a53a68c1d49eb7e11","modified":1614670343625},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"5237a368ab99123749d724b6c379415f2c142a96","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"34b88784ec120dfdc20fa82aadeb5f64ef614d14","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"fb23b85db6f7d8279d73ae1f41631f92f64fc864","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"d3f03be036b75dc71cf3c366cd75aee7c127c874","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"ca1030efdfca5e20f9db2e7a428998e66a24c0d0","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"8b18f84503a361fc712b0fe4d4568e2f086ca97d","modified":1614670343625},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"9c8dc0b8170679cdc1ee9ee8dbcbaebf3f42897b","modified":1614670343625},{"_id":"themes/next/layout/_macro/post.swig","hash":"090b5a9b6fca8e968178004cbd6cff205b7eba57","modified":1614670343625},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"71655ca21907e9061b6e8ac52d0d8fbf54d0062b","modified":1614670343625},{"_id":"themes/next/layout/_partials/comments.swig","hash":"db6ab5421b5f4b7cb32ac73ad0e053fdf065f83e","modified":1614670343625},{"_id":"themes/next/layout/_partials/footer.swig","hash":"3b84596d26514d2e77138172701ef99f97a9226a","modified":1614673363502},{"_id":"themes/next/layout/_partials/languages.swig","hash":"ba9e272f1065b8f0e8848648caa7dea3f02c6be1","modified":1614670343625},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1614670343625},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1614670343625},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1614670343625},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1614670343625},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"4d2c93c66e069852bb0e3ea2e268d213d07bfa3f","modified":1614670343625},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1614670343625},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1614670343625},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"b782eb2e34c0c15440837040b5d65b093ab6ec04","modified":1614670343625},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1614670343625},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"311e5eceec9e949f1ea8d623b083cec0b8700ff2","modified":1614670343625},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"2731e262a6b88eaee2a3ca61e6a3583a7f594702","modified":1614670343625},{"_id":"themes/next/scripts/events/index.js","hash":"5743cde07f3d2aa11532a168a652e52ec28514fd","modified":1614670343629},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1614670343629},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1614670343629},{"_id":"themes/next/scripts/filters/locals.js","hash":"b193a936ee63451f09f8886343dcfdca577c0141","modified":1614670343629},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1614670343629},{"_id":"themes/next/scripts/filters/post.js","hash":"44ba9b1c0bdda57590b53141306bb90adf0678db","modified":1614670343629},{"_id":"themes/next/scripts/helpers/engine.js","hash":"bdb424c3cc0d145bd0c6015bb1d2443c8a9c6cda","modified":1614670343629},{"_id":"themes/next/scripts/helpers/font.js","hash":"40cf00e9f2b7aa6e5f33d412e03ed10304b15fd7","modified":1614670343629},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"5e11f30ddb5093a88a687446617a46b048fa02e5","modified":1614670343629},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"958e86b2bd24e4fdfcbf9ce73e998efe3491a71f","modified":1614670343629},{"_id":"themes/next/scripts/tags/button.js","hash":"8c6b45f36e324820c919a822674703769e6da32c","modified":1614670343629},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"94e0bbc7999b359baa42fa3731bdcf89c79ae2b3","modified":1614670343629},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f1826ade2d135e2f60e2d95cb035383685b3370c","modified":1614670343629},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1614670343629},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1614670343629},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1614670343629},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1614670343629},{"_id":"themes/next/scripts/tags/pdf.js","hash":"8c613b39e7bff735473e35244b5629d02ee20618","modified":1614670343629},{"_id":"themes/next/scripts/tags/tabs.js","hash":"93d8a734a3035c1d3f04933167b500517557ba3e","modified":1614670343629},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1614670343629},{"_id":"themes/next/source/css/_colors.styl","hash":"a8442520f719d3d7a19811cb3b85bcfd4a596e1f","modified":1614670343629},{"_id":"themes/next/source/css/_mixins.styl","hash":"e31a557f8879c2f4d8d5567ee1800b3e03f91f6e","modified":1614670343629},{"_id":"themes/next/source/css/main.styl","hash":"a3a3bbb5a973052f0186b3523911cb2539ff7b88","modified":1614670343629},{"_id":"themes/next/docs/zh-CN/README.md","hash":"c038629ff8f3f24e8593c4c8ecf0bef3a35c750d","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"b92585d251f1f9ebe401abb5d932cb920f9b8b10","modified":1614670343625},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"d9ce7331c1236bbe0a551d56cef2405e47e65325","modified":1614670343625},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1614670343629},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1614670343629},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1614670343629},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1614670343629},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1614670343629},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1614670343629},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1614670343629},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1614670343629},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1614670343629},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1614670343629},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1614670343629},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1614670343629},{"_id":"themes/next/source/images/logo-16x16.png","hash":"7bc2dd15dd417ceafe28775f868b8bb977d096a8","modified":1614697652533},{"_id":"themes/next/source/images/logo-32x32.png","hash":"735ea6a5012400f5a35c105304022c1a6ad00dd1","modified":1614697632224},{"_id":"themes/next/source/images/logo.png","hash":"67a8b593dc8a6be9623aa6204543ec834079ff7e","modified":1614697587722},{"_id":"themes/next/source/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1614670343629},{"_id":"themes/next/source/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1614670343629},{"_id":"themes/next/source/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1614670343629},{"_id":"themes/next/source/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1614670343629},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1614670343629},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"000bad572d76ee95d9c0a78f9ccdc8d97cc7d4b4","modified":1614670343625},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"810d544019e4a8651b756dd23e5592ee851eda71","modified":1614670343625},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"c70f8e71e026e878a4e9d5ab3bbbf9b0b23c240c","modified":1614670343625},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"7dbe93b8297b746afb89700b4d29289556e85267","modified":1614670343625},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"9440d8a3a181698b80e1fa47f5104f4565d8cdf3","modified":1614670343625},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"d31f896680a6c2f2c3f5128b4d4dd46c87ce2130","modified":1614670343625},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"ae2261bea836581918a1c2b0d1028a78718434e0","modified":1614670343625},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1614670343625},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"9b7a66791d7822c52117fe167612265356512477","modified":1614670343625},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"954ad71536b6eb08bd1f30ac6e2f5493b69d1c04","modified":1614670343625},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"ceba16b9bd3a0c5c8811af7e7e49d0f9dcb2f41e","modified":1614670343625},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1614670343625},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1614670343625},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"2b1a73556595c37951e39574df5a3f20b2edeaef","modified":1614670343625},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"48430bd03b8f19c9b8cdb2642005ed67d56c6e0b","modified":1614670343625},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"2be50f9bfb1c56b85b3b6910a7df27f51143632c","modified":1614670343625},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"f48a6a8eba04eb962470ce76dd731e13074d4c45","modified":1614670343625},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"c46849e0af8f8fb78baccd40d2af14df04a074af","modified":1614670343625},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"077b5d66f6309f2e7dcf08645058ff2e03143e6c","modified":1614670343625},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1614670343625},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1614670343625},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1614670343625},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1614670343625},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1614670343625},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1614670343625},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1614670343625},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1614670343625},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"f910618292c63871ca2e6c6e66c491f344fa7b1f","modified":1614670343625},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1614670343625},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1614670343625},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b14908644225d78c864cd0a9b60c52407de56183","modified":1614670343625},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"82f5b6822aa5ec958aa987b101ef860494c6cf1f","modified":1614670343625},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"d6ceb70648555338a80ae5724b778c8c58d7060d","modified":1614670343625},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1614670343625},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"be0a8eccf1f6dc21154af297fc79555343031277","modified":1614670343625},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1614670343625},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1614670343625},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"ecf751321e799f0fb3bf94d049e535130e2547aa","modified":1614670343625},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"d35a999d67f4c302f76fdf13744ceef3c6506481","modified":1614670343625},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1614670343629},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1614670343629},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"4b1986e43d6abce13450d2b41a736dd6a5620a10","modified":1614670343629},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1614670343629},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"b26ac2bfbe91dd88267f8b96aee6bb222b265b7a","modified":1614670343629},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1614670343629},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"d56d5af427cdfecc33a0f62ee62c056b4e33d095","modified":1614670343629},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"f3c43664a071ff3c0b28bd7e59b5523446829576","modified":1614670343629},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"d30b0e255a8092043bac46441243f943ed6fb09b","modified":1614670343629},{"_id":"themes/next/scripts/events/lib/config.js","hash":"d34c6040b13649714939f59be5175e137de65ede","modified":1614670343629},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1614670343629},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"a54708fd9309b4357c423a3730eb67f395344a5e","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"2486f3e0150c753e5f3af1a3665d074704b8ee2c","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"4c0c99c7e0f00849003dfce02a131104fb671137","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1614670343629},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"6cbd85f9433c06bae22225ccf75ac55e04f2d106","modified":1614670343629},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"f4e694e5db81e57442c7e34505a416d818b3044a","modified":1614670343629},{"_id":"themes/next/source/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1614670343629},{"_id":"themes/next/source/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1614670343629},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"612ec843372dae709acb17112c1145a53450cc59","modified":1614670343629},{"_id":"themes/next/source/css/_variables/base.styl","hash":"818508748b7a62e02035e87fe58e75b603ed56dc","modified":1614670343629},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"f70be8e229da7e1715c11dd0e975a2e71e453ac8","modified":1614670343629},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1614670343629},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1614670343633},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1614670343633},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"ca5e70662dcfb261c25191cc5db5084dcf661c76","modified":1614670343629},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"62df49459d552bbf73841753da8011a1f5e875c8","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"8e7b57a72e757cf95278239641726bb2d5b869d1","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"a47725574e1bee3bc3b63b0ff2039cc982b17eff","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"2e3bf7baf383c9073ec5e67f157d3cb3823c0957","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"a1690e035b505d28bdef2b4424c13fc6312ab049","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"0b2c4b78eead410020d7c4ded59c75592a648df8","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"a2e9e00962e43e98ec2614d6d248ef1773bb9b78","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"b1f0fab7344a20ed6748b04065b141ad423cf4d9","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"18ce72d90459c9aa66910ac64eae115f2dde3767","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"7785bd756e0c4acede3a47fec1ed7b55988385a5","modified":1614670343629},{"_id":"themes/next/source/images/logo.svg","hash":"17c7579190a955a679853ae845df3430a80e49c5","modified":1614697561839},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"681d33e3bc85bdca407d93b134c089264837378c","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"f6516d0f7d89dc7b6c6e143a5af54b926f585d82","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"bb7ace23345364eb14983e860a7172e1683a4c94","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"7104b9cef90ca3b140d7a7afcf15540a250218fc","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"6136da4bbb7e70cec99f5c7ae8c7e74f5e7c261a","modified":1614670343629},{"_id":"themes/next/source/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"f0131db6275ceaecae7e1a6a3798b8f89f6c850d","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"93db5dafe9294542a6b5f647643cb9deaced8e06","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"e282df938bd029f391c466168d0e68389978f120","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"70a4324b70501132855b5e59029acfc5d3da1ebd","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"85da2f3006f4bef9a2199416ecfab4d288f848c4","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"44f47c88c06d89d06f220f102649057118715828","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"e740deadcfc4f29c5cb01e40f9df6277262ba4e3","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1614670343629},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2b2e7b5cea7783c9c8bb92655e26a67c266886f0","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1614670343629},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"e771dcb0b4673e063c0f3e2d73e7336ac05bcd57","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"d21d4ac1982c13d02f125a67c065412085a92ff2","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"e75693f33dbc92afc55489438267869ae2f3db54","modified":1614670343629},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"902569a9dea90548bec21a823dd3efd94ff7c133","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ded41fd9d20a5e8db66aaff7cc50f105f5ef2952","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f49ca072b5a800f735e8f01fc3518f885951dd8e","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"65cb6edb69e94e70e3291e9132408361148d41d5","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"6a97bcfa635d637dc59005be3b931109e0d1ead5","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"d114b2a531129e739a27ba6271cfe6857aa9a865","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"a760ee83ba6216871a9f14c5e56dc9bd0d9e2103","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"1e4190c10c9e0c9ce92653b0dbcec21754b0b69d","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"b49e9fbd3c182b8fc066b8c2caf248e3eb748619","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"9f0b93d109c9aec79450c8a0cf4a4eab717d674d","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"454a4aebfabb4469b92a8cbb49f46c49ac9bf165","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"a793cfff86ad4af818faef04c18013077873f8f0","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"5f432a6ed9ca80a413c68b00e93d4a411abf280a","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"45a239edca44acecf971d99b04f30a1aafbf6906","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b2fc519828fe89a1f8f03ff7b809ad68cd46f3d7","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"fa0222197b5eee47e18ac864cdc6eac75678b8fe","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"44487d9ab290dc97871fa8dd4487016deb56e123","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"1f0e7fbe80956f47087c2458ea880acf7a83078b","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"a960a2dd587b15d3b3fe1b59525d6fa971c6a6ec","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"a05a4031e799bc864a4536f9ef61fe643cd421af","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"a9cd93c36bae5af9223e7804963096274e8a4f03","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2a47f8a6bb589c2fb635e6c1e4a2563c7f63c407","modified":1614670343629},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"35c871a809afa8306c8cde13651010e282548bc6","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"1d2778ca5aeeeafaa690dc2766b01b352ab76a02","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"d7fce4b51b5f4b7c31d93a9edb6c6ce740aa0d6b","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"e4d9a77ffe98e851c1202676940097ba28253313","modified":1614670343629},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e2d606f1ac343e9be4f15dbbaf3464bc4df8bf81","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"f23670f1d8e749f3e83766d446790d8fd9620278","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1614670343629},{"_id":"source/images/全世界无产者联合起来.gif","hash":"73d2532b5a14c0d59d3154341e914d5f36e43659","modified":1614693109173},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"e7a9fdb6478b8674b1cdf94de4f8052843fb71d9","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f71a3e86c05ea668b008cf05a81f67d92b6d65e4","modified":1614670343629},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b49c64f8e9a6ca1c45c0ba98febf1974fdd03616","modified":1614670343629},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1614670343629},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1614670343633},{"_id":"source/_posts/无标度网络的生成模型/degree_dist.png","hash":"ac884847d07e5035a54dc504b48b13b810d0e26e","modified":1614692496761},{"_id":"source/images/bg.jpeg","hash":"58780454697332581af2e22bd411d79e1b8d9ce4","modified":1614698432663},{"_id":"public/search.xml","hash":"eee04d7dada3669d6a212d4349448e98c9040000","modified":1616511930038},{"_id":"public/about/index.html","hash":"ce40e657f23fbd3393237d1a66bdefa8c0d80237","modified":1616511930038},{"_id":"public/categories/index.html","hash":"e7852b4e811b23396c01564919e9cc32d1371a45","modified":1616511930038},{"_id":"public/tags/index.html","hash":"b7cb01b341ac944e5967ec6314d28d7d5080f043","modified":1616511930038},{"_id":"public/archives/index.html","hash":"680118017de4811acd4ff610c8be7909913ee283","modified":1616511930038},{"_id":"public/archives/2021/index.html","hash":"d150d3e319d54b035dddab1c446075991e181f57","modified":1616511930038},{"_id":"public/archives/2021/02/index.html","hash":"991eed44c88032f2adc7e015b580acc28bde1b1f","modified":1616511930038},{"_id":"public/archives/2021/03/index.html","hash":"18098db0092d7bb38c2b2d0495773521bdad559c","modified":1616511930038},{"_id":"public/categories/神经网络/index.html","hash":"b9e7c7d459f05b930b9b9ed52c9747b1fde0c2f0","modified":1616511930038},{"_id":"public/categories/复杂网络/index.html","hash":"71c0df34df805091ab125107e2805e29d09bf82c","modified":1616511930038},{"_id":"public/categories/极限环/index.html","hash":"09649863b93699bd53d2f474ed01befd3c6d88d4","modified":1616511930038},{"_id":"public/categories/计算方法/index.html","hash":"83b4e77705b2fe6e5cdf71cdcccb156224b11b32","modified":1616511930038},{"_id":"public/tags/RNN/index.html","hash":"32584c8149c21814f9f6aa83346ea039a64344ba","modified":1616511930038},{"_id":"public/tags/BPDC/index.html","hash":"a1611166eaab15b50a512b7d2ff536a791575888","modified":1616511930038},{"_id":"public/tags/训练/index.html","hash":"2d7f3304da7c5b2f2996d1ef1ff4364391ec4c77","modified":1616511930038},{"_id":"public/tags/神经网络/index.html","hash":"9f82775d0acb7a40b8d2739ee4fdb22a43339c84","modified":1616511930038},{"_id":"public/tags/无标度/index.html","hash":"82b0b7cef891ef229df688798f8977433a36b8c5","modified":1616511930038},{"_id":"public/tags/网络/index.html","hash":"ac4700da1ae286787df8aca20206a176257ddae8","modified":1616511930038},{"_id":"public/tags/极限环/index.html","hash":"a0613e3b047277842397caa9eb7614ee4a33b401","modified":1616511930038},{"_id":"public/tags/matlab/index.html","hash":"281705652a29401c3c0bdbc2fdfff95d11524aa6","modified":1616511930038},{"_id":"public/tags/导数/index.html","hash":"4b8cc1629b3892d2cbf6e6204c3b003d5a6110b3","modified":1616511930038},{"_id":"public/tags/中心差分/index.html","hash":"00e2879a8bbc06990177b9b3cecfc43d51c6823c","modified":1616511930038},{"_id":"public/2021/03/计算一阶导数的四阶中心差分格式/index.html","hash":"3846a2d313aee6016ed63bbd8ba5fc4436baac9b","modified":1616511930038},{"_id":"public/2021/03/构造极限环/index.html","hash":"fced551f2f497e3de0d91b7ba3d6976fe95aa386","modified":1616511930038},{"_id":"public/2021/03/BPDC/index.html","hash":"c28e7b1b55cc0be500b5d06f6929f174205b09ec","modified":1616511930038},{"_id":"public/2021/02/无标度网络的生成模型/index.html","hash":"f90b28edc771a1a17c37e66343dd86e72732845b","modified":1616511930038},{"_id":"public/index.html","hash":"a72342fa7b6a757715ee33c3c3e2a7ef146ab04f","modified":1616511930038},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1616511930038},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1616511930038},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1616511930038},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1616511930038},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1616511930038},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1616511930038},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1616511930038},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1616511930038},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1616511930038},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1616511930038},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1616511930038},{"_id":"public/images/logo-16x16.png","hash":"7bc2dd15dd417ceafe28775f868b8bb977d096a8","modified":1616511930038},{"_id":"public/images/logo-32x32.png","hash":"735ea6a5012400f5a35c105304022c1a6ad00dd1","modified":1616511930038},{"_id":"public/images/logo.png","hash":"67a8b593dc8a6be9623aa6204543ec834079ff7e","modified":1616511930038},{"_id":"public/images/logo.svg","hash":"17c7579190a955a679853ae845df3430a80e49c5","modified":1616511930038},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1616511930038},{"_id":"public/images/logo-header.png","hash":"651a9b5148d097e7498b27bb2e502ea10e0041b4","modified":1616511930038},{"_id":"public/images/世界人民大团结万岁.gif","hash":"7cf242aea54914d5d12d0046e6b8eecc80eaac59","modified":1616511930038},{"_id":"public/2021/03/BPDC/RNN.png","hash":"b3771ba252ccf1c2a1553f141e49110341ac7305","modified":1616511930038},{"_id":"public/2021/03/构造极限环/pic1.png","hash":"c12b5e363eed8e5f40e8b0c65af7edf15ac526c2","modified":1616511930038},{"_id":"public/2021/03/构造极限环/pic2.png","hash":"aa7187703b6507cd7ed895f14c62a6ae5428d1d4","modified":1616511930038},{"_id":"public/2021/03/计算一阶导数的四阶中心差分格式/四阶中心差分.png","hash":"5164571d132ab26494d5986f465193d6c91ac8d7","modified":1616511930038},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1616511930038},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1616511930038},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1616511930038},{"_id":"public/images/alen.png","hash":"6898877141e21218a781fd932112defe5ea23e05","modified":1616511930038},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1616511930038},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1616511930038},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1616511930038},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1616511930038},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1616511930038},{"_id":"public/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1616511930038},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1616511930038},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1616511930038},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1616511930038},{"_id":"public/css/main.css","hash":"11e3e6bed4fb11372782db101e00393aa475acd6","modified":1616511930038},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1616511930038},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1616511930038},{"_id":"public/images/全世界无产者联合起来.gif","hash":"73d2532b5a14c0d59d3154341e914d5f36e43659","modified":1616511930038},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1616511930038},{"_id":"public/2021/02/无标度网络的生成模型/degree_dist.png","hash":"ac884847d07e5035a54dc504b48b13b810d0e26e","modified":1616511930038},{"_id":"public/images/bg.jpeg","hash":"58780454697332581af2e22bd411d79e1b8d9ce4","modified":1616511930038}],"Category":[{"name":"神经网络","_id":"ckmm5gsqd0004kzj8bwld7lzt"},{"name":"复杂网络","_id":"ckmm5gsqg0009kzj8bgeh82ea"},{"name":"极限环","_id":"ckmm5gsqh000ckzj8dl5o7xjr"},{"name":"计算方法","_id":"ckmm5gsqh000fkzj8e0zxd64c"}],"Data":[{"_id":"styles","data":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}],"Page":[{"title":"about","date":"2021-02-24T08:30:00.000Z","_content":"\nWho is your daddy?\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-02-24 16:30:00\n---\n\nWho is your daddy?\n","updated":"2021-02-24T08:38:48.533Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckmm5gsq60000kzj834vx91lz","content":"<p>Who is your daddy?</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"","more":"<p>Who is your daddy?</p>\n"},{"title":"categories","date":"2021-03-02T08:17:00.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-03-02 16:17:00\ntype: categories\n---\n","updated":"2021-03-02T08:17:58.270Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckmm5gsqb0002kzj83pitgoag","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"","more":""},{"title":"tags","date":"2021-03-02T08:18:35.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-03-02 16:18:35\ntype: tags\n---\n","updated":"2021-03-02T08:19:15.966Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckmm5gsqe0006kzj8dfxd5fjp","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"","more":""}],"Post":[{"title":"RNN 训练算法 —— BPDC (Backpropagation-Decorrelation)","date":"2021-03-23T11:30:22.000Z","mathjax":true,"_content":"\n# 问题描述\n考虑模型循环网络模型：\n$$\nx(k+1) = (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] \\tag1{}\n$$\n其中 $x(k) \\in R^N$表示网络节点在激活前的状态，$W\\in R^{N\\times N}$表示网络结点之间相互连接的权重，网络的输出节点为 $\\{x_i(k)| i\\in O\\}$，$O$为所有输出（或称“观测”）单元的下标集合\n\n训练的目标是为了减少观测状态和预期值之间误差，即最小化损失函数：\n$$\nE = \\frac{1}{2}\\sum_{k=1}^K \\sum_{i\\in O} [x_i(k) - d_i(k)]^2 \\tag{2}\n$$\n其中 $d_i(k)$ 表示 $k$ 时刻第 $i$ 个节点的预期值\n\n<!--more-->\n\n![RNN](RNN.png)\n\n# 符号约定\n$$\nW \\equiv\n\\begin{bmatrix}\n\\text{-----}  w_1^T \\text{-----} \\\\\n\\vdots \\\\\n\\text{-----}  w_N^T \\text{-----} \n\\end{bmatrix}_{N\\times N}\n$$\n将矩阵 $W$ 拉成列向量，记为 $w$\n$$\nw = [w_1^T, \\cdots, w_N^T]^T \\in R^{N^2}\n$$\n把所有时间的状态拼成列向量，记为 $x$\n$$\nx = [x^T(1), \\cdots, x^T(K)]^T \\in R^{NK}\n$$\n将RNN 的训练视为约束优化问题，(1)式转化成约束条件:\n$$\ng(k+1) \\equiv  -x(k+1) + (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] , \\quad k=1,\\ldots ,K \\tag{3}\n$$\n记\n$$\ng = [g^T(1), \\ldots, g^T(K)]^T \\in R^{NK}\n$$\n\n\n# Atiya-Parlos 算法回顾\n以上是经典的梯度下降法的思维，但是 Atiya-Parlos 提出了另一种优化思路：不是朝着参数的梯度方向更新，但仍使代价函数下降\n\n该算法的思想是互换网络状态 $x(k)$ 和权重矩阵 $W$ 的作用：将状态视为控制变量，并根据 $x(k)$ 的变化确定权重的变化。 换句话说，我们计算 $E$ 相对于状态 $x(k)$ 的梯度，并假设状态在该梯度的负方向 $\\displaystyle{\\Delta x_i(k) = -\\eta\\frac{\\partial E}{\\partial x_i(k)} }$ 上有微小变化。\n\n接下来，我们确定权重 $W$ 的变化 $\\Delta w$，以使由权重变化导致的状态变化尽可能地接近目标变化 $\\Delta x$\n\n该算法的细节如下：\n$$\n\\begin{aligned}\n\\Delta x &= -\\eta \\left(\\frac{\\partial E}{\\partial x_i} \\right)^T \\\\\n&= -\\eta e^T\\\\\n&= -\\eta [e(1), \\ldots, e(K)]^T \\\\\\\\\ne_i(k)&= \\begin{cases}\n   x_i(k) - d_i(k), &\\text{if } i\\in O, \\\\\n   0, &\\text{otherwise. } \n\\end{cases} \nk \\in 1,\\ldots,K.\n\\end{aligned}\n$$\n\n\n由约束条件得：\n$$\n\\frac{\\partial g}{\\partial x} \\Delta x = - \\frac{\\partial g}{\\partial w} \\Delta w\n$$\n故已知 $\\Delta x$ 时，可得：\n$$\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x\n$$\n需要注意逆矩阵不一定存在，故\n$$\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I \\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x\n$$\n这就是权重 $W$ 的更新规则\n\n# 计算细节\n\n计算 $\\frac{\\partial g}{\\partial w}$\n$$\n\\frac{\\partial g}{\\partial w} =\n \\begin{bmatrix}\n\\frac{\\partial g(1)}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial g(K)}{\\partial w}\n\\end{bmatrix}\n=  \\Delta t \\begin{bmatrix}\n\\frac{\\partial  Wf[x(0)] }{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial Wf[x(K-1)] }{\\partial w}\n\\end{bmatrix}\n$$\n\n其中\n$$\n\\begin{aligned}\n\\frac{\\partial Wf[x(k)]}{\\partial w}\n&= \\begin{bmatrix}\n\\frac{\\partial w_1^Tf[x(k)]}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial w_N^Tf[x(k)]}{\\partial w}\n\\end{bmatrix} \\color{red}{记 f_k = [f(x_1), \\ldots, f(x_N(k))]^T}\\\\\\\\\n& = \\begin{bmatrix}\nf_k^T &&& \\\\\n & f_k^T&& \\\\\n && \\ddots & \\\\\n &&& f_k^T\n\\end{bmatrix}_{N\\times N^2} \\\\\\\\\n&\\triangleq  F(k)\n\\end{aligned}\n$$\n\n故\n$$\n\\frac{\\partial g}{\\partial w} = \n\\Delta t \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix}_{NK \\times N^2}\n$$\n$$\n\\begin{aligned}\n&\\frac{1}{\\Delta t^2}\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)  \\\\\n&= \n \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}\n \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix} \\\\\\\\\n&= \n\\sum_{k=0}^{K-1} F^T(k)F(k) \\\\\\\\\n&=\\begin{bmatrix}\n \\sum_{k=0}^{K-1} f_k f_k^T &&& \\\\\n &  \\sum_{k=0}^{K-1} f_k f_k^T  && \\\\\n && \\ddots & \\\\\n &&&  \\sum_{k=0}^{K-1} f_k f_k^T \n\\end{bmatrix}_{N^2 \\times N^2} \\\\\\\\\n&\\triangleq diag\\{C_{K-1}\\}\n\\end{aligned}\n$$\n\n\n令\n$$\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x \n$$\n$\\gamma$ 表示由 $\\Delta x$ 提供的误差信息，它的计算放在本文最后，先假设它已经求出来了\n\n则\n$$\n\\begin{aligned}\n& \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x  \\\\\n&= \n\\Delta t \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}_{N^2 \\times NK}\n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK} \\\\\n&= \\Delta t\\sum_{k=1}^K F^T(k-1)\\gamma(k) \\\\\n&=\\Delta t \\sum_{k=1}^K   \\begin{bmatrix}\nf_{k-1} &&& \\\\\n & f_{k-1}&& \\\\\n && \\ddots & \\\\\n &&& f_{k-1}\n\\end{bmatrix}_{N^2 \\times N}\n\\begin{bmatrix}\n\\gamma_1(k)\\\\ \n\\gamma_2(k) \\\\ \n\\vdots \\\\\n \\gamma_N(k)\n\\end{bmatrix}_{N}\\\\\\\\\n&=\\Delta t \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\end{aligned} \n$$\n\n所以\n$$\n\\begin{aligned}\n\\Delta w \n&=  -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x \\\\\n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\Delta W \n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} \\\\\n&= - \\frac{1}{\\Delta t}  \\sum_{k=1}^K\\begin{bmatrix}\n f_{k-1}^T \\gamma_1(k)\\\\ \n f_{k-1}^T \\gamma_2(k) \\\\ \n\\vdots \\\\\nf_{k-1}^T \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} C_{K-1}^{-1} \\\\ \n\\end{aligned} \n$$\n\n其中\n$$\nC_{K-1} = \\epsilon I + \\sum_{r=0}^{K-1} f_r f_r^T\n$$\n注意：上述 $\\Delta W$ 是基于 $1,2,\\ldots, K$ 整个时间段的更新，不妨称之为 $\\Delta W_{batch}$\n\n下面将更新公式拆解在线更新（online updating）的形式：\n$$\n\\Delta W^{batch}(K)= \\Delta W(1) + \\cdots + \\Delta W(K)\n$$\n\n等式右端对应每一时刻的更新量\n\n在第 $K$ 时刻的第 $i$ 个神经元的输入权重的更新量：\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(K) &=  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K} f_{k-1}^TC_{K-1}^{-1}  \\gamma_i(k) + \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^TC_{K-2}^{-1}  \\gamma_i(k)\\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T (C_{K-1}^{-1} - C_{K-2}^{-1}) \\gamma_i(k) \\\\\\\\\n&=- \\frac{1}{\\Delta t}   f_{K-1}^T C_{K-1}^{-1}\\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T C_{K-2}^{-1}\\gamma_i(k)(C_{K-2}C_{K-1}^{-1} - I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -   \\Delta w_i^{batch}(K-1)(C_{K-2}C_{K-1}^{-1}- I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -  \\sum_{k=1}^{K-1} \\Delta w^T_i(k)  (C_{K-2}C_{K-1}^{-1}- I)\n\\end{aligned}\n$$\n可以看出，APRL 的更新规则由当前时刻的误差和 w 的累计更新（动量）组成\n\n随着 $K \\to \\infty$，易知$\\sum_{k=1}^{K-1} \\Delta w^T_i(k) \\to const, C_{K-2}C_{K-1}^{-1} \\to I$，所以第二项趋于零\n\n# BPDC 更新规则\nBPDC 对 APRL 的在线算法做了简单粗暴的近似\n\n该近似不试图累积完整的相关矩阵 $C_k$，也舍弃了先前误差的累积，而且只计算瞬时相关 $C(k)$：\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^TC(k)^{-1}  \\gamma_i(k+1)  \\\\\\\\\nC(k) &= \\epsilon I + f_k f_k^T\n\\end{aligned}\n$$\n利用[矩阵求逆引理](https://blog.csdn.net/itnerd/article/details/105612704)：\n$$\n\\begin{aligned}\nC(k)^{-1} &= (\\epsilon I + f_k f_k^T)^{-1} \\\\\n&= \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\n\\end{aligned}\n$$\n所以\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^T\\left( \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\\right)  \\gamma_i(k+1)  \\\\\\\\\n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\gamma_i(k+1)  \n\\end{aligned}\n$$\n\n# 计算 $\\gamma$\n$$\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x = -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T\n$$\n关键在与计算 $\\frac{\\partial g}{\\partial x}$\n$$\n\\begin{aligned}\n\\frac{\\partial g}{\\partial x} &= \n \\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_1}{\\partial x(K)}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial g_K}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_K}{\\partial x(K)}\n\\end{bmatrix}\\\\\\\\\n&=  \n\\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & 0&  0 &\\ldots & 0\\\\\n\\frac{\\partial g_2}{\\partial x(1)} & \\frac{\\partial g_2}{\\partial x(2)}& 0 &\\ldots & 0  \\\\\n0 & \\frac{\\partial g_3}{\\partial x(2)} & \\frac{\\partial g_3}{\\partial x(3)} & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\frac{\\partial g_K}{\\partial x(K-1)}& \\frac{\\partial g_K}{\\partial x(K)}\n\\end{bmatrix} \\\\\\\\\n&=  \n\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t W D(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n其中\n$$\nD(k) =  \\begin{bmatrix}\n f'(x_1(k)) & \\cdots&0\\\\\n\\vdots & \\ddots & \\vdots\\\\\n0& \\ldots &  f'(x_N(k))\n\\end{bmatrix}_{N \\times N}\n$$\n所以\n$$\n\\begin{aligned}\n\\gamma &= -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T\\\\\n&= -\\eta\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t WD(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\begin{bmatrix}\ne(1) \\\\\ne(2)  \\\\\ne(3) \\\\\n\\vdots  \\\\\ne(K)\n\\end{bmatrix} \\\\\\\\\n&= \n-\\eta \\begin{bmatrix}\n-e(1) \\\\\n[(1-\\Delta t )I + \\Delta t W D(1)]e(1) - e(2)  \\\\\n[(1-\\Delta t )I + \\Delta t W D(2)]e(2) - e(3)  \\\\\n\\vdots  \\\\\n[(1-\\Delta t )I + \\Delta t W D(K-1)]e(K-1) - e(K) \n\\end{bmatrix}_{NK \\times 1}\n\\end{aligned}\n$$\n代入到 BPDC 更新规则：\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) \n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\color{red}{ \\gamma_i(k+1)}  \\\\\n&= \\frac{\\color{red}{\\eta}}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf} \\color{red}\\{ (1-\\Delta t )e_i(k) + \\Delta t \\sum_{s\\in O}w_{is} f'(x_s(k))e_s(k) - e_i(k+1) \\} \n\\end{aligned}\n$$\n# 参考文献\n- J.J. Steil, Backpropagation-decorrelation: online recurrent learning with O(N) complexity, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN), vol. 1, 2004, pp. 843–848.\n- J.J. Steil, Online stability of backpropagation-decorrelation recurrent learning, Neurocomputing 69 (2006) 642–650.\n- J.J. Steil, Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning, Neural Networks 20 (3) (2007) 353–364.\n\n","source":"_posts/BPDC.md","raw":"---\ntitle: RNN 训练算法 —— BPDC (Backpropagation-Decorrelation)\ndate: 2021-03-23 19:30:22\ntags: [RNN, BPDC, 训练, 神经网络]\ncategories: 神经网络\nmathjax: true\n---\n\n# 问题描述\n考虑模型循环网络模型：\n$$\nx(k+1) = (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] \\tag1{}\n$$\n其中 $x(k) \\in R^N$表示网络节点在激活前的状态，$W\\in R^{N\\times N}$表示网络结点之间相互连接的权重，网络的输出节点为 $\\{x_i(k)| i\\in O\\}$，$O$为所有输出（或称“观测”）单元的下标集合\n\n训练的目标是为了减少观测状态和预期值之间误差，即最小化损失函数：\n$$\nE = \\frac{1}{2}\\sum_{k=1}^K \\sum_{i\\in O} [x_i(k) - d_i(k)]^2 \\tag{2}\n$$\n其中 $d_i(k)$ 表示 $k$ 时刻第 $i$ 个节点的预期值\n\n<!--more-->\n\n![RNN](RNN.png)\n\n# 符号约定\n$$\nW \\equiv\n\\begin{bmatrix}\n\\text{-----}  w_1^T \\text{-----} \\\\\n\\vdots \\\\\n\\text{-----}  w_N^T \\text{-----} \n\\end{bmatrix}_{N\\times N}\n$$\n将矩阵 $W$ 拉成列向量，记为 $w$\n$$\nw = [w_1^T, \\cdots, w_N^T]^T \\in R^{N^2}\n$$\n把所有时间的状态拼成列向量，记为 $x$\n$$\nx = [x^T(1), \\cdots, x^T(K)]^T \\in R^{NK}\n$$\n将RNN 的训练视为约束优化问题，(1)式转化成约束条件:\n$$\ng(k+1) \\equiv  -x(k+1) + (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] , \\quad k=1,\\ldots ,K \\tag{3}\n$$\n记\n$$\ng = [g^T(1), \\ldots, g^T(K)]^T \\in R^{NK}\n$$\n\n\n# Atiya-Parlos 算法回顾\n以上是经典的梯度下降法的思维，但是 Atiya-Parlos 提出了另一种优化思路：不是朝着参数的梯度方向更新，但仍使代价函数下降\n\n该算法的思想是互换网络状态 $x(k)$ 和权重矩阵 $W$ 的作用：将状态视为控制变量，并根据 $x(k)$ 的变化确定权重的变化。 换句话说，我们计算 $E$ 相对于状态 $x(k)$ 的梯度，并假设状态在该梯度的负方向 $\\displaystyle{\\Delta x_i(k) = -\\eta\\frac{\\partial E}{\\partial x_i(k)} }$ 上有微小变化。\n\n接下来，我们确定权重 $W$ 的变化 $\\Delta w$，以使由权重变化导致的状态变化尽可能地接近目标变化 $\\Delta x$\n\n该算法的细节如下：\n$$\n\\begin{aligned}\n\\Delta x &= -\\eta \\left(\\frac{\\partial E}{\\partial x_i} \\right)^T \\\\\n&= -\\eta e^T\\\\\n&= -\\eta [e(1), \\ldots, e(K)]^T \\\\\\\\\ne_i(k)&= \\begin{cases}\n   x_i(k) - d_i(k), &\\text{if } i\\in O, \\\\\n   0, &\\text{otherwise. } \n\\end{cases} \nk \\in 1,\\ldots,K.\n\\end{aligned}\n$$\n\n\n由约束条件得：\n$$\n\\frac{\\partial g}{\\partial x} \\Delta x = - \\frac{\\partial g}{\\partial w} \\Delta w\n$$\n故已知 $\\Delta x$ 时，可得：\n$$\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x\n$$\n需要注意逆矩阵不一定存在，故\n$$\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I \\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x\n$$\n这就是权重 $W$ 的更新规则\n\n# 计算细节\n\n计算 $\\frac{\\partial g}{\\partial w}$\n$$\n\\frac{\\partial g}{\\partial w} =\n \\begin{bmatrix}\n\\frac{\\partial g(1)}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial g(K)}{\\partial w}\n\\end{bmatrix}\n=  \\Delta t \\begin{bmatrix}\n\\frac{\\partial  Wf[x(0)] }{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial Wf[x(K-1)] }{\\partial w}\n\\end{bmatrix}\n$$\n\n其中\n$$\n\\begin{aligned}\n\\frac{\\partial Wf[x(k)]}{\\partial w}\n&= \\begin{bmatrix}\n\\frac{\\partial w_1^Tf[x(k)]}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial w_N^Tf[x(k)]}{\\partial w}\n\\end{bmatrix} \\color{red}{记 f_k = [f(x_1), \\ldots, f(x_N(k))]^T}\\\\\\\\\n& = \\begin{bmatrix}\nf_k^T &&& \\\\\n & f_k^T&& \\\\\n && \\ddots & \\\\\n &&& f_k^T\n\\end{bmatrix}_{N\\times N^2} \\\\\\\\\n&\\triangleq  F(k)\n\\end{aligned}\n$$\n\n故\n$$\n\\frac{\\partial g}{\\partial w} = \n\\Delta t \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix}_{NK \\times N^2}\n$$\n$$\n\\begin{aligned}\n&\\frac{1}{\\Delta t^2}\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)  \\\\\n&= \n \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}\n \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix} \\\\\\\\\n&= \n\\sum_{k=0}^{K-1} F^T(k)F(k) \\\\\\\\\n&=\\begin{bmatrix}\n \\sum_{k=0}^{K-1} f_k f_k^T &&& \\\\\n &  \\sum_{k=0}^{K-1} f_k f_k^T  && \\\\\n && \\ddots & \\\\\n &&&  \\sum_{k=0}^{K-1} f_k f_k^T \n\\end{bmatrix}_{N^2 \\times N^2} \\\\\\\\\n&\\triangleq diag\\{C_{K-1}\\}\n\\end{aligned}\n$$\n\n\n令\n$$\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x \n$$\n$\\gamma$ 表示由 $\\Delta x$ 提供的误差信息，它的计算放在本文最后，先假设它已经求出来了\n\n则\n$$\n\\begin{aligned}\n& \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x  \\\\\n&= \n\\Delta t \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}_{N^2 \\times NK}\n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK} \\\\\n&= \\Delta t\\sum_{k=1}^K F^T(k-1)\\gamma(k) \\\\\n&=\\Delta t \\sum_{k=1}^K   \\begin{bmatrix}\nf_{k-1} &&& \\\\\n & f_{k-1}&& \\\\\n && \\ddots & \\\\\n &&& f_{k-1}\n\\end{bmatrix}_{N^2 \\times N}\n\\begin{bmatrix}\n\\gamma_1(k)\\\\ \n\\gamma_2(k) \\\\ \n\\vdots \\\\\n \\gamma_N(k)\n\\end{bmatrix}_{N}\\\\\\\\\n&=\\Delta t \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\end{aligned} \n$$\n\n所以\n$$\n\\begin{aligned}\n\\Delta w \n&=  -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x \\\\\n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\Delta W \n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} \\\\\n&= - \\frac{1}{\\Delta t}  \\sum_{k=1}^K\\begin{bmatrix}\n f_{k-1}^T \\gamma_1(k)\\\\ \n f_{k-1}^T \\gamma_2(k) \\\\ \n\\vdots \\\\\nf_{k-1}^T \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} C_{K-1}^{-1} \\\\ \n\\end{aligned} \n$$\n\n其中\n$$\nC_{K-1} = \\epsilon I + \\sum_{r=0}^{K-1} f_r f_r^T\n$$\n注意：上述 $\\Delta W$ 是基于 $1,2,\\ldots, K$ 整个时间段的更新，不妨称之为 $\\Delta W_{batch}$\n\n下面将更新公式拆解在线更新（online updating）的形式：\n$$\n\\Delta W^{batch}(K)= \\Delta W(1) + \\cdots + \\Delta W(K)\n$$\n\n等式右端对应每一时刻的更新量\n\n在第 $K$ 时刻的第 $i$ 个神经元的输入权重的更新量：\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(K) &=  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K} f_{k-1}^TC_{K-1}^{-1}  \\gamma_i(k) + \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^TC_{K-2}^{-1}  \\gamma_i(k)\\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T (C_{K-1}^{-1} - C_{K-2}^{-1}) \\gamma_i(k) \\\\\\\\\n&=- \\frac{1}{\\Delta t}   f_{K-1}^T C_{K-1}^{-1}\\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T C_{K-2}^{-1}\\gamma_i(k)(C_{K-2}C_{K-1}^{-1} - I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -   \\Delta w_i^{batch}(K-1)(C_{K-2}C_{K-1}^{-1}- I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -  \\sum_{k=1}^{K-1} \\Delta w^T_i(k)  (C_{K-2}C_{K-1}^{-1}- I)\n\\end{aligned}\n$$\n可以看出，APRL 的更新规则由当前时刻的误差和 w 的累计更新（动量）组成\n\n随着 $K \\to \\infty$，易知$\\sum_{k=1}^{K-1} \\Delta w^T_i(k) \\to const, C_{K-2}C_{K-1}^{-1} \\to I$，所以第二项趋于零\n\n# BPDC 更新规则\nBPDC 对 APRL 的在线算法做了简单粗暴的近似\n\n该近似不试图累积完整的相关矩阵 $C_k$，也舍弃了先前误差的累积，而且只计算瞬时相关 $C(k)$：\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^TC(k)^{-1}  \\gamma_i(k+1)  \\\\\\\\\nC(k) &= \\epsilon I + f_k f_k^T\n\\end{aligned}\n$$\n利用[矩阵求逆引理](https://blog.csdn.net/itnerd/article/details/105612704)：\n$$\n\\begin{aligned}\nC(k)^{-1} &= (\\epsilon I + f_k f_k^T)^{-1} \\\\\n&= \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\n\\end{aligned}\n$$\n所以\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^T\\left( \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\\right)  \\gamma_i(k+1)  \\\\\\\\\n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\gamma_i(k+1)  \n\\end{aligned}\n$$\n\n# 计算 $\\gamma$\n$$\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x = -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T\n$$\n关键在与计算 $\\frac{\\partial g}{\\partial x}$\n$$\n\\begin{aligned}\n\\frac{\\partial g}{\\partial x} &= \n \\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_1}{\\partial x(K)}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial g_K}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_K}{\\partial x(K)}\n\\end{bmatrix}\\\\\\\\\n&=  \n\\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & 0&  0 &\\ldots & 0\\\\\n\\frac{\\partial g_2}{\\partial x(1)} & \\frac{\\partial g_2}{\\partial x(2)}& 0 &\\ldots & 0  \\\\\n0 & \\frac{\\partial g_3}{\\partial x(2)} & \\frac{\\partial g_3}{\\partial x(3)} & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\frac{\\partial g_K}{\\partial x(K-1)}& \\frac{\\partial g_K}{\\partial x(K)}\n\\end{bmatrix} \\\\\\\\\n&=  \n\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t W D(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n其中\n$$\nD(k) =  \\begin{bmatrix}\n f'(x_1(k)) & \\cdots&0\\\\\n\\vdots & \\ddots & \\vdots\\\\\n0& \\ldots &  f'(x_N(k))\n\\end{bmatrix}_{N \\times N}\n$$\n所以\n$$\n\\begin{aligned}\n\\gamma &= -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T\\\\\n&= -\\eta\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t WD(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\begin{bmatrix}\ne(1) \\\\\ne(2)  \\\\\ne(3) \\\\\n\\vdots  \\\\\ne(K)\n\\end{bmatrix} \\\\\\\\\n&= \n-\\eta \\begin{bmatrix}\n-e(1) \\\\\n[(1-\\Delta t )I + \\Delta t W D(1)]e(1) - e(2)  \\\\\n[(1-\\Delta t )I + \\Delta t W D(2)]e(2) - e(3)  \\\\\n\\vdots  \\\\\n[(1-\\Delta t )I + \\Delta t W D(K-1)]e(K-1) - e(K) \n\\end{bmatrix}_{NK \\times 1}\n\\end{aligned}\n$$\n代入到 BPDC 更新规则：\n$$\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) \n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\color{red}{ \\gamma_i(k+1)}  \\\\\n&= \\frac{\\color{red}{\\eta}}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf} \\color{red}\\{ (1-\\Delta t )e_i(k) + \\Delta t \\sum_{s\\in O}w_{is} f'(x_s(k))e_s(k) - e_i(k+1) \\} \n\\end{aligned}\n$$\n# 参考文献\n- J.J. Steil, Backpropagation-decorrelation: online recurrent learning with O(N) complexity, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN), vol. 1, 2004, pp. 843–848.\n- J.J. Steil, Online stability of backpropagation-decorrelation recurrent learning, Neurocomputing 69 (2006) 642–650.\n- J.J. Steil, Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning, Neural Networks 20 (3) (2007) 353–364.\n\n","slug":"BPDC","published":1,"updated":"2021-03-23T11:44:51.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmm5gsq90001kzj8fxizhdb4","content":"<h1 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h1><p>考虑模型循环网络模型：</p>\n<script type=\"math/tex; mode=display\">\nx(k+1) = (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] \\tag1{}</script><p>其中 $x(k) \\in R^N$表示网络节点在激活前的状态，$W\\in R^{N\\times N}$表示网络结点之间相互连接的权重，网络的输出节点为 $\\{x_i(k)| i\\in O\\}$，$O$为所有输出（或称“观测”）单元的下标集合</p>\n<p>训练的目标是为了减少观测状态和预期值之间误差，即最小化损失函数：</p>\n<script type=\"math/tex; mode=display\">\nE = \\frac{1}{2}\\sum_{k=1}^K \\sum_{i\\in O} [x_i(k) - d_i(k)]^2 \\tag{2}</script><p>其中 $d_i(k)$ 表示 $k$ 时刻第 $i$ 个节点的预期值</p>\n<span id=\"more\"></span>\n<p><img src=\"RNN.png\" alt=\"RNN\"></p>\n<h1 id=\"符号约定\"><a href=\"#符号约定\" class=\"headerlink\" title=\"符号约定\"></a>符号约定</h1><script type=\"math/tex; mode=display\">\nW \\equiv\n\\begin{bmatrix}\n\\text{-----}  w_1^T \\text{-----} \\\\\n\\vdots \\\\\n\\text{-----}  w_N^T \\text{-----} \n\\end{bmatrix}_{N\\times N}</script><p>将矩阵 $W$ 拉成列向量，记为 $w$</p>\n<script type=\"math/tex; mode=display\">\nw = [w_1^T, \\cdots, w_N^T]^T \\in R^{N^2}</script><p>把所有时间的状态拼成列向量，记为 $x$</p>\n<script type=\"math/tex; mode=display\">\nx = [x^T(1), \\cdots, x^T(K)]^T \\in R^{NK}</script><p>将RNN 的训练视为约束优化问题，(1)式转化成约束条件:</p>\n<script type=\"math/tex; mode=display\">\ng(k+1) \\equiv  -x(k+1) + (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] , \\quad k=1,\\ldots ,K \\tag{3}</script><p>记</p>\n<script type=\"math/tex; mode=display\">\ng = [g^T(1), \\ldots, g^T(K)]^T \\in R^{NK}</script><h1 id=\"Atiya-Parlos-算法回顾\"><a href=\"#Atiya-Parlos-算法回顾\" class=\"headerlink\" title=\"Atiya-Parlos 算法回顾\"></a>Atiya-Parlos 算法回顾</h1><p>以上是经典的梯度下降法的思维，但是 Atiya-Parlos 提出了另一种优化思路：不是朝着参数的梯度方向更新，但仍使代价函数下降</p>\n<p>该算法的思想是互换网络状态 $x(k)$ 和权重矩阵 $W$ 的作用：将状态视为控制变量，并根据 $x(k)$ 的变化确定权重的变化。 换句话说，我们计算 $E$ 相对于状态 $x(k)$ 的梯度，并假设状态在该梯度的负方向 $\\displaystyle{\\Delta x_i(k) = -\\eta\\frac{\\partial E}{\\partial x_i(k)} }$ 上有微小变化。</p>\n<p>接下来，我们确定权重 $W$ 的变化 $\\Delta w$，以使由权重变化导致的状态变化尽可能地接近目标变化 $\\Delta x$</p>\n<p>该算法的细节如下：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta x &= -\\eta \\left(\\frac{\\partial E}{\\partial x_i} \\right)^T \\\\\n&= -\\eta e^T\\\\\n&= -\\eta [e(1), \\ldots, e(K)]^T \\\\\\\\\ne_i(k)&= \\begin{cases}\n   x_i(k) - d_i(k), &\\text{if } i\\in O, \\\\\n   0, &\\text{otherwise. } \n\\end{cases} \nk \\in 1,\\ldots,K.\n\\end{aligned}</script><p>由约束条件得：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial g}{\\partial x} \\Delta x = - \\frac{\\partial g}{\\partial w} \\Delta w</script><p>故已知 $\\Delta x$ 时，可得：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x</script><p>需要注意逆矩阵不一定存在，故</p>\n<script type=\"math/tex; mode=display\">\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I \\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x</script><p>这就是权重 $W$ 的更新规则</p>\n<h1 id=\"计算细节\"><a href=\"#计算细节\" class=\"headerlink\" title=\"计算细节\"></a>计算细节</h1><p>计算 $\\frac{\\partial g}{\\partial w}$</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial g}{\\partial w} =\n \\begin{bmatrix}\n\\frac{\\partial g(1)}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial g(K)}{\\partial w}\n\\end{bmatrix}\n=  \\Delta t \\begin{bmatrix}\n\\frac{\\partial  Wf[x(0)] }{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial Wf[x(K-1)] }{\\partial w}\n\\end{bmatrix}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial Wf[x(k)]}{\\partial w}\n&= \\begin{bmatrix}\n\\frac{\\partial w_1^Tf[x(k)]}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial w_N^Tf[x(k)]}{\\partial w}\n\\end{bmatrix} \\color{red}{记 f_k = [f(x_1), \\ldots, f(x_N(k))]^T}\\\\\\\\\n& = \\begin{bmatrix}\nf_k^T &&& \\\\\n & f_k^T&& \\\\\n && \\ddots & \\\\\n &&& f_k^T\n\\end{bmatrix}_{N\\times N^2} \\\\\\\\\n&\\triangleq  F(k)\n\\end{aligned}</script><p>故</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial g}{\\partial w} = \n\\Delta t \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix}_{NK \\times N^2}</script><script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&\\frac{1}{\\Delta t^2}\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)  \\\\\n&= \n \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}\n \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix} \\\\\\\\\n&= \n\\sum_{k=0}^{K-1} F^T(k)F(k) \\\\\\\\\n&=\\begin{bmatrix}\n \\sum_{k=0}^{K-1} f_k f_k^T &&& \\\\\n &  \\sum_{k=0}^{K-1} f_k f_k^T  && \\\\\n && \\ddots & \\\\\n &&&  \\sum_{k=0}^{K-1} f_k f_k^T \n\\end{bmatrix}_{N^2 \\times N^2} \\\\\\\\\n&\\triangleq diag\\{C_{K-1}\\}\n\\end{aligned}</script><p>令</p>\n<script type=\"math/tex; mode=display\">\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x</script><p>$\\gamma$ 表示由 $\\Delta x$ 提供的误差信息，它的计算放在本文最后，先假设它已经求出来了</p>\n<p>则</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n& \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x  \\\\\n&= \n\\Delta t \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}_{N^2 \\times NK}\n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK} \\\\\n&= \\Delta t\\sum_{k=1}^K F^T(k-1)\\gamma(k) \\\\\n&=\\Delta t \\sum_{k=1}^K   \\begin{bmatrix}\nf_{k-1} &&& \\\\\n & f_{k-1}&& \\\\\n && \\ddots & \\\\\n &&& f_{k-1}\n\\end{bmatrix}_{N^2 \\times N}\n\\begin{bmatrix}\n\\gamma_1(k)\\\\ \n\\gamma_2(k) \\\\ \n\\vdots \\\\\n \\gamma_N(k)\n\\end{bmatrix}_{N}\\\\\\\\\n&=\\Delta t \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\end{aligned}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w \n&=  -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x \\\\\n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\Delta W \n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} \\\\\n&= - \\frac{1}{\\Delta t}  \\sum_{k=1}^K\\begin{bmatrix}\n f_{k-1}^T \\gamma_1(k)\\\\ \n f_{k-1}^T \\gamma_2(k) \\\\ \n\\vdots \\\\\nf_{k-1}^T \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} C_{K-1}^{-1} \\\\ \n\\end{aligned}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\nC_{K-1} = \\epsilon I + \\sum_{r=0}^{K-1} f_r f_r^T</script><p>注意：上述 $\\Delta W$ 是基于 $1,2,\\ldots, K$ 整个时间段的更新，不妨称之为 $\\Delta W_{batch}$</p>\n<p>下面将更新公式拆解在线更新（online updating）的形式：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta W^{batch}(K)= \\Delta W(1) + \\cdots + \\Delta W(K)</script><p>等式右端对应每一时刻的更新量</p>\n<p>在第 $K$ 时刻的第 $i$ 个神经元的输入权重的更新量：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(K) &=  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K} f_{k-1}^TC_{K-1}^{-1}  \\gamma_i(k) + \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^TC_{K-2}^{-1}  \\gamma_i(k)\\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T (C_{K-1}^{-1} - C_{K-2}^{-1}) \\gamma_i(k) \\\\\\\\\n&=- \\frac{1}{\\Delta t}   f_{K-1}^T C_{K-1}^{-1}\\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T C_{K-2}^{-1}\\gamma_i(k)(C_{K-2}C_{K-1}^{-1} - I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -   \\Delta w_i^{batch}(K-1)(C_{K-2}C_{K-1}^{-1}- I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -  \\sum_{k=1}^{K-1} \\Delta w^T_i(k)  (C_{K-2}C_{K-1}^{-1}- I)\n\\end{aligned}</script><p>可以看出，APRL 的更新规则由当前时刻的误差和 w 的累计更新（动量）组成</p>\n<p>随着 $K \\to \\infty$，易知$\\sum_{k=1}^{K-1} \\Delta w^T_i(k) \\to const, C_{K-2}C_{K-1}^{-1} \\to I$，所以第二项趋于零</p>\n<h1 id=\"BPDC-更新规则\"><a href=\"#BPDC-更新规则\" class=\"headerlink\" title=\"BPDC 更新规则\"></a>BPDC 更新规则</h1><p>BPDC 对 APRL 的在线算法做了简单粗暴的近似</p>\n<p>该近似不试图累积完整的相关矩阵 $C_k$，也舍弃了先前误差的累积，而且只计算瞬时相关 $C(k)$：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^TC(k)^{-1}  \\gamma_i(k+1)  \\\\\\\\\nC(k) &= \\epsilon I + f_k f_k^T\n\\end{aligned}</script><p>利用<a href=\"https://blog.csdn.net/itnerd/article/details/105612704\">矩阵求逆引理</a>：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nC(k)^{-1} &= (\\epsilon I + f_k f_k^T)^{-1} \\\\\n&= \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\n\\end{aligned}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^T\\left( \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\\right)  \\gamma_i(k+1)  \\\\\\\\\n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\gamma_i(k+1)  \n\\end{aligned}</script><h1 id=\"计算-gamma\"><a href=\"#计算-gamma\" class=\"headerlink\" title=\"计算 $\\gamma$\"></a>计算 $\\gamma$</h1><script type=\"math/tex; mode=display\">\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x = -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T</script><p>关键在与计算 $\\frac{\\partial g}{\\partial x}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial g}{\\partial x} &= \n \\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_1}{\\partial x(K)}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial g_K}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_K}{\\partial x(K)}\n\\end{bmatrix}\\\\\\\\\n&=  \n\\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & 0&  0 &\\ldots & 0\\\\\n\\frac{\\partial g_2}{\\partial x(1)} & \\frac{\\partial g_2}{\\partial x(2)}& 0 &\\ldots & 0  \\\\\n0 & \\frac{\\partial g_3}{\\partial x(2)} & \\frac{\\partial g_3}{\\partial x(3)} & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\frac{\\partial g_K}{\\partial x(K-1)}& \\frac{\\partial g_K}{\\partial x(K)}\n\\end{bmatrix} \\\\\\\\\n&=  \n\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t W D(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\end{aligned}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\nD(k) =  \\begin{bmatrix}\n f'(x_1(k)) & \\cdots&0\\\\\n\\vdots & \\ddots & \\vdots\\\\\n0& \\ldots &  f'(x_N(k))\n\\end{bmatrix}_{N \\times N}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\gamma &= -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T\\\\\n&= -\\eta\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t WD(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\begin{bmatrix}\ne(1) \\\\\ne(2)  \\\\\ne(3) \\\\\n\\vdots  \\\\\ne(K)\n\\end{bmatrix} \\\\\\\\\n&= \n-\\eta \\begin{bmatrix}\n-e(1) \\\\\n[(1-\\Delta t )I + \\Delta t W D(1)]e(1) - e(2)  \\\\\n[(1-\\Delta t )I + \\Delta t W D(2)]e(2) - e(3)  \\\\\n\\vdots  \\\\\n[(1-\\Delta t )I + \\Delta t W D(K-1)]e(K-1) - e(K) \n\\end{bmatrix}_{NK \\times 1}\n\\end{aligned}</script><p>代入到 BPDC 更新规则：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) \n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\color{red}{ \\gamma_i(k+1)}  \\\\\n&= \\frac{\\color{red}{\\eta}}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf} \\color{red}\\{ (1-\\Delta t )e_i(k) + \\Delta t \\sum_{s\\in O}w_{is} f'(x_s(k))e_s(k) - e_i(k+1) \\} \n\\end{aligned}</script><h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><ul>\n<li>J.J. Steil, Backpropagation-decorrelation: online recurrent learning with O(N) complexity, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN), vol. 1, 2004, pp. 843–848.</li>\n<li>J.J. Steil, Online stability of backpropagation-decorrelation recurrent learning, Neurocomputing 69 (2006) 642–650.</li>\n<li>J.J. Steil, Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning, Neural Networks 20 (3) (2007) 353–364.</li>\n</ul>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"<h1 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h1><p>考虑模型循环网络模型：</p>\n<script type=\"math/tex; mode=display\">\nx(k+1) = (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] \\tag1{}</script><p>其中 $x(k) \\in R^N$表示网络节点在激活前的状态，$W\\in R^{N\\times N}$表示网络结点之间相互连接的权重，网络的输出节点为 $\\{x_i(k)| i\\in O\\}$，$O$为所有输出（或称“观测”）单元的下标集合</p>\n<p>训练的目标是为了减少观测状态和预期值之间误差，即最小化损失函数：</p>\n<script type=\"math/tex; mode=display\">\nE = \\frac{1}{2}\\sum_{k=1}^K \\sum_{i\\in O} [x_i(k) - d_i(k)]^2 \\tag{2}</script><p>其中 $d_i(k)$ 表示 $k$ 时刻第 $i$ 个节点的预期值</p>","more":"<p><img src=\"RNN.png\" alt=\"RNN\"></p>\n<h1 id=\"符号约定\"><a href=\"#符号约定\" class=\"headerlink\" title=\"符号约定\"></a>符号约定</h1><script type=\"math/tex; mode=display\">\nW \\equiv\n\\begin{bmatrix}\n\\text{-----}  w_1^T \\text{-----} \\\\\n\\vdots \\\\\n\\text{-----}  w_N^T \\text{-----} \n\\end{bmatrix}_{N\\times N}</script><p>将矩阵 $W$ 拉成列向量，记为 $w$</p>\n<script type=\"math/tex; mode=display\">\nw = [w_1^T, \\cdots, w_N^T]^T \\in R^{N^2}</script><p>把所有时间的状态拼成列向量，记为 $x$</p>\n<script type=\"math/tex; mode=display\">\nx = [x^T(1), \\cdots, x^T(K)]^T \\in R^{NK}</script><p>将RNN 的训练视为约束优化问题，(1)式转化成约束条件:</p>\n<script type=\"math/tex; mode=display\">\ng(k+1) \\equiv  -x(k+1) + (1-\\Delta t)x(k) +  \\Delta t Wf[x(k)] , \\quad k=1,\\ldots ,K \\tag{3}</script><p>记</p>\n<script type=\"math/tex; mode=display\">\ng = [g^T(1), \\ldots, g^T(K)]^T \\in R^{NK}</script><h1 id=\"Atiya-Parlos-算法回顾\"><a href=\"#Atiya-Parlos-算法回顾\" class=\"headerlink\" title=\"Atiya-Parlos 算法回顾\"></a>Atiya-Parlos 算法回顾</h1><p>以上是经典的梯度下降法的思维，但是 Atiya-Parlos 提出了另一种优化思路：不是朝着参数的梯度方向更新，但仍使代价函数下降</p>\n<p>该算法的思想是互换网络状态 $x(k)$ 和权重矩阵 $W$ 的作用：将状态视为控制变量，并根据 $x(k)$ 的变化确定权重的变化。 换句话说，我们计算 $E$ 相对于状态 $x(k)$ 的梯度，并假设状态在该梯度的负方向 $\\displaystyle{\\Delta x_i(k) = -\\eta\\frac{\\partial E}{\\partial x_i(k)} }$ 上有微小变化。</p>\n<p>接下来，我们确定权重 $W$ 的变化 $\\Delta w$，以使由权重变化导致的状态变化尽可能地接近目标变化 $\\Delta x$</p>\n<p>该算法的细节如下：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta x &= -\\eta \\left(\\frac{\\partial E}{\\partial x_i} \\right)^T \\\\\n&= -\\eta e^T\\\\\n&= -\\eta [e(1), \\ldots, e(K)]^T \\\\\\\\\ne_i(k)&= \\begin{cases}\n   x_i(k) - d_i(k), &\\text{if } i\\in O, \\\\\n   0, &\\text{otherwise. } \n\\end{cases} \nk \\in 1,\\ldots,K.\n\\end{aligned}</script><p>由约束条件得：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial g}{\\partial x} \\Delta x = - \\frac{\\partial g}{\\partial w} \\Delta w</script><p>故已知 $\\Delta x$ 时，可得：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x</script><p>需要注意逆矩阵不一定存在，故</p>\n<script type=\"math/tex; mode=display\">\n\\Delta w = -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I \\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x</script><p>这就是权重 $W$ 的更新规则</p>\n<h1 id=\"计算细节\"><a href=\"#计算细节\" class=\"headerlink\" title=\"计算细节\"></a>计算细节</h1><p>计算 $\\frac{\\partial g}{\\partial w}$</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial g}{\\partial w} =\n \\begin{bmatrix}\n\\frac{\\partial g(1)}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial g(K)}{\\partial w}\n\\end{bmatrix}\n=  \\Delta t \\begin{bmatrix}\n\\frac{\\partial  Wf[x(0)] }{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial Wf[x(K-1)] }{\\partial w}\n\\end{bmatrix}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial Wf[x(k)]}{\\partial w}\n&= \\begin{bmatrix}\n\\frac{\\partial w_1^Tf[x(k)]}{\\partial w}\\\\\n\\vdots \\\\\n\\frac{\\partial w_N^Tf[x(k)]}{\\partial w}\n\\end{bmatrix} \\color{red}{记 f_k = [f(x_1), \\ldots, f(x_N(k))]^T}\\\\\\\\\n& = \\begin{bmatrix}\nf_k^T &&& \\\\\n & f_k^T&& \\\\\n && \\ddots & \\\\\n &&& f_k^T\n\\end{bmatrix}_{N\\times N^2} \\\\\\\\\n&\\triangleq  F(k)\n\\end{aligned}</script><p>故</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial g}{\\partial w} = \n\\Delta t \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix}_{NK \\times N^2}</script><script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&\\frac{1}{\\Delta t^2}\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right)  \\\\\n&= \n \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}\n \\begin{bmatrix}\nF(0)\\\\\n\\vdots \\\\\nF(K-1)\n\\end{bmatrix} \\\\\\\\\n&= \n\\sum_{k=0}^{K-1} F^T(k)F(k) \\\\\\\\\n&=\\begin{bmatrix}\n \\sum_{k=0}^{K-1} f_k f_k^T &&& \\\\\n &  \\sum_{k=0}^{K-1} f_k f_k^T  && \\\\\n && \\ddots & \\\\\n &&&  \\sum_{k=0}^{K-1} f_k f_k^T \n\\end{bmatrix}_{N^2 \\times N^2} \\\\\\\\\n&\\triangleq diag\\{C_{K-1}\\}\n\\end{aligned}</script><p>令</p>\n<script type=\"math/tex; mode=display\">\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x</script><p>$\\gamma$ 表示由 $\\Delta x$ 提供的误差信息，它的计算放在本文最后，先假设它已经求出来了</p>\n<p>则</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n& \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x  \\\\\n&= \n\\Delta t \\begin{bmatrix}\nF^T(0) &\n\\cdots &\nF^T(K-1)\n\\end{bmatrix}_{N^2 \\times NK}\n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK} \\\\\n&= \\Delta t\\sum_{k=1}^K F^T(k-1)\\gamma(k) \\\\\n&=\\Delta t \\sum_{k=1}^K   \\begin{bmatrix}\nf_{k-1} &&& \\\\\n & f_{k-1}&& \\\\\n && \\ddots & \\\\\n &&& f_{k-1}\n\\end{bmatrix}_{N^2 \\times N}\n\\begin{bmatrix}\n\\gamma_1(k)\\\\ \n\\gamma_2(k) \\\\ \n\\vdots \\\\\n \\gamma_N(k)\n\\end{bmatrix}_{N}\\\\\\\\\n&=\\Delta t \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\end{aligned}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w \n&=  -\\left[\\left(\\frac{\\partial g}{\\partial w}\\right)^T \\left(\\frac{\\partial g}{\\partial w}\\right) + \\epsilon I\\right]^{-1} \\left(\\frac{\\partial g}{\\partial w}\\right)^T\\left( \\frac{\\partial g}{\\partial x}\\right) \\Delta x \\\\\n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_1(k)\\\\ \nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\nC_{K-1}^{-1} \\sum_{k=1}^K f_{k-1} \\gamma_N(k)\n\\end{bmatrix}_{N^2}\\\\\\\\\n\\Delta W \n&= - \\frac{1}{\\Delta t} \\begin{bmatrix}\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_1(k)\\\\ \n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_2(k) \\\\ \n\\vdots \\\\\n \\sum_{k=1}^K f_{k-1}^TC_{K-1}^{-1} \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} \\\\\n&= - \\frac{1}{\\Delta t}  \\sum_{k=1}^K\\begin{bmatrix}\n f_{k-1}^T \\gamma_1(k)\\\\ \n f_{k-1}^T \\gamma_2(k) \\\\ \n\\vdots \\\\\nf_{k-1}^T \\gamma_N(k)\n\\end{bmatrix}_{N\\times N} C_{K-1}^{-1} \\\\ \n\\end{aligned}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\nC_{K-1} = \\epsilon I + \\sum_{r=0}^{K-1} f_r f_r^T</script><p>注意：上述 $\\Delta W$ 是基于 $1,2,\\ldots, K$ 整个时间段的更新，不妨称之为 $\\Delta W_{batch}$</p>\n<p>下面将更新公式拆解在线更新（online updating）的形式：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta W^{batch}(K)= \\Delta W(1) + \\cdots + \\Delta W(K)</script><p>等式右端对应每一时刻的更新量</p>\n<p>在第 $K$ 时刻的第 $i$ 个神经元的输入权重的更新量：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(K) &=  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K} f_{k-1}^TC_{K-1}^{-1}  \\gamma_i(k) + \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^TC_{K-2}^{-1}  \\gamma_i(k)\\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T (C_{K-1}^{-1} - C_{K-2}^{-1}) \\gamma_i(k) \\\\\\\\\n&=- \\frac{1}{\\Delta t}   f_{K-1}^T C_{K-1}^{-1}\\gamma_i(K)  - \\frac{1}{\\Delta t} \\sum_{k=1}^{K-1} f_{k-1}^T C_{K-2}^{-1}\\gamma_i(k)(C_{K-2}C_{K-1}^{-1} - I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -   \\Delta w_i^{batch}(K-1)(C_{K-2}C_{K-1}^{-1}- I) \\\\\\\\\n&= - \\frac{1}{\\Delta t}   f_{K-1}^TC_{K-1}^{-1} \\gamma_i(K)  -  \\sum_{k=1}^{K-1} \\Delta w^T_i(k)  (C_{K-2}C_{K-1}^{-1}- I)\n\\end{aligned}</script><p>可以看出，APRL 的更新规则由当前时刻的误差和 w 的累计更新（动量）组成</p>\n<p>随着 $K \\to \\infty$，易知$\\sum_{k=1}^{K-1} \\Delta w^T_i(k) \\to const, C_{K-2}C_{K-1}^{-1} \\to I$，所以第二项趋于零</p>\n<h1 id=\"BPDC-更新规则\"><a href=\"#BPDC-更新规则\" class=\"headerlink\" title=\"BPDC 更新规则\"></a>BPDC 更新规则</h1><p>BPDC 对 APRL 的在线算法做了简单粗暴的近似</p>\n<p>该近似不试图累积完整的相关矩阵 $C_k$，也舍弃了先前误差的累积，而且只计算瞬时相关 $C(k)$：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^TC(k)^{-1}  \\gamma_i(k+1)  \\\\\\\\\nC(k) &= \\epsilon I + f_k f_k^T\n\\end{aligned}</script><p>利用<a href=\"https://blog.csdn.net/itnerd/article/details/105612704\">矩阵求逆引理</a>：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nC(k)^{-1} &= (\\epsilon I + f_k f_k^T)^{-1} \\\\\n&= \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\n\\end{aligned}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) &= - \\frac{1}{\\Delta t}  f_{k}^T\\left( \\frac{1}{\\epsilon}I - \\frac{1}{\\epsilon} \\frac{ff^T}{\\epsilon + f^Tf}\\right)  \\gamma_i(k+1)  \\\\\\\\\n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\gamma_i(k+1)  \n\\end{aligned}</script><h1 id=\"计算-gamma\"><a href=\"#计算-gamma\" class=\"headerlink\" title=\"计算 $\\gamma$\"></a>计算 $\\gamma$</h1><script type=\"math/tex; mode=display\">\n\\gamma = \n\\begin{bmatrix}\n\\gamma(1)\\\\ \n\\gamma(2) \\\\ \n\\vdots \\\\\n \\gamma(K)\n\\end{bmatrix}_{NK}\n= \\frac{\\partial g}{\\partial x} \\Delta x = -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T</script><p>关键在与计算 $\\frac{\\partial g}{\\partial x}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial g}{\\partial x} &= \n \\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_1}{\\partial x(K)}\\\\\n\\vdots & \\ddots & \\vdots\\\\\n\\frac{\\partial g_K}{\\partial x(1)} & \\ldots & \\frac{ \\partial g_K}{\\partial x(K)}\n\\end{bmatrix}\\\\\\\\\n&=  \n\\begin{bmatrix}\n\\frac{\\partial g_1}{\\partial x(1)} & 0&  0 &\\ldots & 0\\\\\n\\frac{\\partial g_2}{\\partial x(1)} & \\frac{\\partial g_2}{\\partial x(2)}& 0 &\\ldots & 0  \\\\\n0 & \\frac{\\partial g_3}{\\partial x(2)} & \\frac{\\partial g_3}{\\partial x(3)} & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\frac{\\partial g_K}{\\partial x(K-1)}& \\frac{\\partial g_K}{\\partial x(K)}\n\\end{bmatrix} \\\\\\\\\n&=  \n\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t W D(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\end{aligned}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">\nD(k) =  \\begin{bmatrix}\n f'(x_1(k)) & \\cdots&0\\\\\n\\vdots & \\ddots & \\vdots\\\\\n0& \\ldots &  f'(x_N(k))\n\\end{bmatrix}_{N \\times N}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\gamma &= -\\eta \\frac{\\partial g}{\\partial x} [e(1), \\ldots, e(K)]^T\\\\\n&= -\\eta\\begin{bmatrix}\n-I & 0&  0 &\\ldots & 0\\\\\n(1-\\Delta t )I + \\Delta t WD(1) & -I& 0 &\\ldots & 0  \\\\\n0 & (1-\\Delta t )I + \\Delta t W D(2) & -I & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 &(1-\\Delta t )I + \\Delta t W D(K-1)& -I\n\\end{bmatrix}\n\\begin{bmatrix}\ne(1) \\\\\ne(2)  \\\\\ne(3) \\\\\n\\vdots  \\\\\ne(K)\n\\end{bmatrix} \\\\\\\\\n&= \n-\\eta \\begin{bmatrix}\n-e(1) \\\\\n[(1-\\Delta t )I + \\Delta t W D(1)]e(1) - e(2)  \\\\\n[(1-\\Delta t )I + \\Delta t W D(2)]e(2) - e(3)  \\\\\n\\vdots  \\\\\n[(1-\\Delta t )I + \\Delta t W D(K-1)]e(K-1) - e(K) \n\\end{bmatrix}_{NK \\times 1}\n\\end{aligned}</script><p>代入到 BPDC 更新规则：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Delta w^T_{i}(k+1) \n&=  - \\frac{1}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf}  \\color{red}{ \\gamma_i(k+1)}  \\\\\n&= \\frac{\\color{red}{\\eta}}{\\Delta t}   \\frac{f^T}{\\epsilon + f^Tf} \\color{red}\\{ (1-\\Delta t )e_i(k) + \\Delta t \\sum_{s\\in O}w_{is} f'(x_s(k))e_s(k) - e_i(k+1) \\} \n\\end{aligned}</script><h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><ul>\n<li>J.J. Steil, Backpropagation-decorrelation: online recurrent learning with O(N) complexity, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN), vol. 1, 2004, pp. 843–848.</li>\n<li>J.J. Steil, Online stability of backpropagation-decorrelation recurrent learning, Neurocomputing 69 (2006) 642–650.</li>\n<li>J.J. Steil, Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning, Neural Networks 20 (3) (2007) 353–364.</li>\n</ul>"},{"layout":"post","title":"无标度网络的生成模型","date":"2021-02-24T06:55:32.000Z","mathjax":true,"_content":"\n1999 年 Barabási 和 Albert 提出了无标度网络模型（简称 BA 模型）。无标度网络的重要特征为： 无标度网络的节点度分布服从幂律分布。\n\n无标度网络的度分布 $p(d)$ 满足$$p(d)\\sim d^{-\\alpha}，$$其中 $d$ 代表度的大小， $\\alpha$ 为度分布的幂律指数。 真实网络 $\\alpha$ 值一般介于 2~3之间。\n\n近年来越来越多的研究表明， 真实世界网络既不是规则网络， 也不是随机网络， 而是兼具小世界和无标度特性的复杂网络， 具有与规则网络和随机网络截然不同的统计特性。\n\n<!--more-->\n\n本文采用的无标度网络生成模型是由 Barabási 和 Albert 于 1999 年提出的增长网络网络模型（BA 模型）。在该模型中，网络初始时具有 $m_0$ 个节点，两两互连。 之后每过一个时间单位增加一个新节点。新节点从当前网络中选择$m(m ≤ m_0)$ 个节点与之连接， 某节点 $v_i$ 被选中的概率 $p(v_i)$ 与其节点度 $d_i$ 的大小成正比，即$$p(v_i) = \\frac{d_i}{\\sum_j d_j}$$经过 t 个时间单位后，网络中含有 $m_0+t$ 个节点，$m_0(m_0-1)/2+mt$条边。可以证明当 t 足够大时， 按此规律增长的网络的度分布为幂指数等于 3 的幂律分布。\n\n依据新节点的连接规律，建立节点度演化的动力学模型：\n$$\n\\begin{aligned}\n\\frac{\\partial d_i}{\\partial t} &= m \\frac{\\partial d_i}{\\sum_j d_j} \\\\\n&= m \\frac{d_i}{2\\left( \\frac{m_0(m_0-1)}{2} + mt\\right)} \\\\\n&= m \\frac{d_i}{\\left( m_0(m_0-1) + 2mt\\right)} \\\\\n&\\simeq \\frac{d_i}{2t}\n\\end{aligned}\n$$\n\n其中最后一个等式在 $t$ 足够大时近似成立。 将节点 $i$ 加入网络的时间记为 $t_i$，\n则有初始条件 $d_i(t_i) = m$。解得\n$$\nd_i = \\left\\{ \n\\begin{array}{ll}\n0, & t < t_i, \\\\\nm(\\frac{t}{t_i})^{0.5}, &t\\geq t_i.\n\\end{array}\n\\right.\n$$\n\n在 $t$ 足够大， 对任意节点 $i$， 其度的大小满足\n$$\n\\begin{aligned}\nP(d_i(t) < d) &= P\\left( t_i > \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - P\\left(t_i \\leq \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - \\frac{m^2 t}{d^2(m_0 + t)} \\\\\n&\\simeq 1- \\frac{m^2}{d^2}\n\\end{aligned}\n$$\n其中第三个等式成立的原因是加入节点的时间是等间隔的。上式正是网络节点度的概率分布函数， 可以求出节点度的概率密度函数 $p(d)$ 为\n$$\np(d) = \\frac{\\partial P(d_i(t) < d)}{\\partial d} = 2\\frac{m^2}{d^3}\n$$\n可知所生成网络的幂律分布的指数为 3。\n\n\n\n下面的matlab程序模拟了BA网络的演化过程：\n\n```matlab\nfunction scale_free(N,m0,m)\n%\n%param  N: num of vertices 期望节点数\n%param m0: num of initial vertices 初始边数\n%param  m: num of vertices a new node try to connect 新节点连接的边数\n%\ntic;\n\nI = 2 ;    %生成的网络个数，只为统计需要\n\nrealization_of_distribution = sparse( I , N ) ;\nfor J = 1 : I\n    format long;\n\n \t%初始化邻接矩阵，前m0个节点两两互连\n    adjacent_matrix = sparse( m0 , m0 ) ;\n    parfor i = 1 : m0\n        for j = 1 : m0\n            if j ~= i\n                adjacent_matrix( i , j ) = 1 ;\n            end\n        end\n    end\n    adjacent_matrix = sparse( adjacent_matrix ) ;\n\n\t% 计算当前节点度分布\n    node_degree = sparse( 1 , m0 ) ;\n    for p = 1 : m0\n        node_degree( p ) = sum( adjacent_matrix( 1 : m0 , p ) ) ;\n    end\n\n\t% 开始演化\n    for iteration = m0 + 1 : N\n        total_degree = 2 * m * ( iteration - m0 -1 ) + m0*(m0-1) ; % m*2\n        degree_frequency = node_degree / total_degree ;\n        cum_distribution = cumsum( degree_frequency ) ;\n\n        choose = zeros( 1 , m ) ;\n        for new_edge = 1:m\n            r = rand(1) ;\n            choose_edge = find( cum_distribution >= r ,1) ;\n            while any(choose == choose_edge)\n                r = rand(1) ;\n                choose_edge = find(  cum_distribution >= r,1) ;\n            end\n            choose(new_edge) = choose_edge;\n        end\n\n        for k = 1 : m\n            adjacent_matrix( iteration , choose(k) ) = 1 ;\n            adjacent_matrix( choose(k) , iteration ) = 1 ;\n        end\n\n        for p = 1 : iteration\n            node_degree(p) = sum( adjacent_matrix( 1 : iteration , p ) ) ;\n        end\n    end\n\n    number_of_nodes_with_equal_degree = zeros( 1 , N ) ;\n\n    parfor i = 1 : N\n        number_of_nodes_with_equal_degree(i) = length( find( node_degree == i ) ) ;\n    end\n    realization_of_distribution( J , : ) = number_of_nodes_with_equal_degree ;\n\n    save(['adj_',num2str(J)],'adjacent_matrix');\nend\n\n%{\n%plot degree distribution 在双对数坐标下画图\n\naverage = sum( realization_of_distribution )/ ( I * N );\nloglog( 1:N , average , '*' )\naxis([1 N 0.0000001 0.9])\nhold on;\nx = 1:N;\ny = 2 * m^2 * x .^ ( -3 ) ;\nloglog( x , y , 'r' ) ;  %  p(k)=2*m^2*k^(-3)\n\n%}\ntoc;\n\nend\n```\n人工生成网络的概率质量函数（网络节点数 $N$ 分别为 50、 100、 200、 400）\n\n![生成网络的节点度分布](degree_dist.png)\n\n图中直线为理论结果： $p(d)=2\\frac{m^2}{d^3}$。\n\n![](/images/世界人民大团结万岁.gif)\n\n\n","source":"_posts/无标度网络的生成模型.md","raw":"---\nlayout: post\ntitle: 无标度网络的生成模型\ndate: 2021-02-24 14:55:32\ntags: [无标度, 网络]\ncategories: 复杂网络\nmathjax: true\n---\n\n1999 年 Barabási 和 Albert 提出了无标度网络模型（简称 BA 模型）。无标度网络的重要特征为： 无标度网络的节点度分布服从幂律分布。\n\n无标度网络的度分布 $p(d)$ 满足$$p(d)\\sim d^{-\\alpha}，$$其中 $d$ 代表度的大小， $\\alpha$ 为度分布的幂律指数。 真实网络 $\\alpha$ 值一般介于 2~3之间。\n\n近年来越来越多的研究表明， 真实世界网络既不是规则网络， 也不是随机网络， 而是兼具小世界和无标度特性的复杂网络， 具有与规则网络和随机网络截然不同的统计特性。\n\n<!--more-->\n\n本文采用的无标度网络生成模型是由 Barabási 和 Albert 于 1999 年提出的增长网络网络模型（BA 模型）。在该模型中，网络初始时具有 $m_0$ 个节点，两两互连。 之后每过一个时间单位增加一个新节点。新节点从当前网络中选择$m(m ≤ m_0)$ 个节点与之连接， 某节点 $v_i$ 被选中的概率 $p(v_i)$ 与其节点度 $d_i$ 的大小成正比，即$$p(v_i) = \\frac{d_i}{\\sum_j d_j}$$经过 t 个时间单位后，网络中含有 $m_0+t$ 个节点，$m_0(m_0-1)/2+mt$条边。可以证明当 t 足够大时， 按此规律增长的网络的度分布为幂指数等于 3 的幂律分布。\n\n依据新节点的连接规律，建立节点度演化的动力学模型：\n$$\n\\begin{aligned}\n\\frac{\\partial d_i}{\\partial t} &= m \\frac{\\partial d_i}{\\sum_j d_j} \\\\\n&= m \\frac{d_i}{2\\left( \\frac{m_0(m_0-1)}{2} + mt\\right)} \\\\\n&= m \\frac{d_i}{\\left( m_0(m_0-1) + 2mt\\right)} \\\\\n&\\simeq \\frac{d_i}{2t}\n\\end{aligned}\n$$\n\n其中最后一个等式在 $t$ 足够大时近似成立。 将节点 $i$ 加入网络的时间记为 $t_i$，\n则有初始条件 $d_i(t_i) = m$。解得\n$$\nd_i = \\left\\{ \n\\begin{array}{ll}\n0, & t < t_i, \\\\\nm(\\frac{t}{t_i})^{0.5}, &t\\geq t_i.\n\\end{array}\n\\right.\n$$\n\n在 $t$ 足够大， 对任意节点 $i$， 其度的大小满足\n$$\n\\begin{aligned}\nP(d_i(t) < d) &= P\\left( t_i > \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - P\\left(t_i \\leq \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - \\frac{m^2 t}{d^2(m_0 + t)} \\\\\n&\\simeq 1- \\frac{m^2}{d^2}\n\\end{aligned}\n$$\n其中第三个等式成立的原因是加入节点的时间是等间隔的。上式正是网络节点度的概率分布函数， 可以求出节点度的概率密度函数 $p(d)$ 为\n$$\np(d) = \\frac{\\partial P(d_i(t) < d)}{\\partial d} = 2\\frac{m^2}{d^3}\n$$\n可知所生成网络的幂律分布的指数为 3。\n\n\n\n下面的matlab程序模拟了BA网络的演化过程：\n\n```matlab\nfunction scale_free(N,m0,m)\n%\n%param  N: num of vertices 期望节点数\n%param m0: num of initial vertices 初始边数\n%param  m: num of vertices a new node try to connect 新节点连接的边数\n%\ntic;\n\nI = 2 ;    %生成的网络个数，只为统计需要\n\nrealization_of_distribution = sparse( I , N ) ;\nfor J = 1 : I\n    format long;\n\n \t%初始化邻接矩阵，前m0个节点两两互连\n    adjacent_matrix = sparse( m0 , m0 ) ;\n    parfor i = 1 : m0\n        for j = 1 : m0\n            if j ~= i\n                adjacent_matrix( i , j ) = 1 ;\n            end\n        end\n    end\n    adjacent_matrix = sparse( adjacent_matrix ) ;\n\n\t% 计算当前节点度分布\n    node_degree = sparse( 1 , m0 ) ;\n    for p = 1 : m0\n        node_degree( p ) = sum( adjacent_matrix( 1 : m0 , p ) ) ;\n    end\n\n\t% 开始演化\n    for iteration = m0 + 1 : N\n        total_degree = 2 * m * ( iteration - m0 -1 ) + m0*(m0-1) ; % m*2\n        degree_frequency = node_degree / total_degree ;\n        cum_distribution = cumsum( degree_frequency ) ;\n\n        choose = zeros( 1 , m ) ;\n        for new_edge = 1:m\n            r = rand(1) ;\n            choose_edge = find( cum_distribution >= r ,1) ;\n            while any(choose == choose_edge)\n                r = rand(1) ;\n                choose_edge = find(  cum_distribution >= r,1) ;\n            end\n            choose(new_edge) = choose_edge;\n        end\n\n        for k = 1 : m\n            adjacent_matrix( iteration , choose(k) ) = 1 ;\n            adjacent_matrix( choose(k) , iteration ) = 1 ;\n        end\n\n        for p = 1 : iteration\n            node_degree(p) = sum( adjacent_matrix( 1 : iteration , p ) ) ;\n        end\n    end\n\n    number_of_nodes_with_equal_degree = zeros( 1 , N ) ;\n\n    parfor i = 1 : N\n        number_of_nodes_with_equal_degree(i) = length( find( node_degree == i ) ) ;\n    end\n    realization_of_distribution( J , : ) = number_of_nodes_with_equal_degree ;\n\n    save(['adj_',num2str(J)],'adjacent_matrix');\nend\n\n%{\n%plot degree distribution 在双对数坐标下画图\n\naverage = sum( realization_of_distribution )/ ( I * N );\nloglog( 1:N , average , '*' )\naxis([1 N 0.0000001 0.9])\nhold on;\nx = 1:N;\ny = 2 * m^2 * x .^ ( -3 ) ;\nloglog( x , y , 'r' ) ;  %  p(k)=2*m^2*k^(-3)\n\n%}\ntoc;\n\nend\n```\n人工生成网络的概率质量函数（网络节点数 $N$ 分别为 50、 100、 200、 400）\n\n![生成网络的节点度分布](degree_dist.png)\n\n图中直线为理论结果： $p(d)=2\\frac{m^2}{d^3}$。\n\n![](/images/世界人民大团结万岁.gif)\n\n\n","slug":"无标度网络的生成模型","published":1,"updated":"2021-03-02T13:57:36.748Z","comments":1,"photos":[],"link":"","_id":"ckmm5gsqb0003kzj81ujf4rm9","content":"<p>1999 年 Barabási 和 Albert 提出了无标度网络模型（简称 BA 模型）。无标度网络的重要特征为： 无标度网络的节点度分布服从幂律分布。</p>\n<p>无标度网络的度分布 $p(d)$ 满足<script type=\"math/tex\">p(d)\\sim d^{-\\alpha}，</script>其中 $d$ 代表度的大小， $\\alpha$ 为度分布的幂律指数。 真实网络 $\\alpha$ 值一般介于 2~3之间。</p>\n<p>近年来越来越多的研究表明， 真实世界网络既不是规则网络， 也不是随机网络， 而是兼具小世界和无标度特性的复杂网络， 具有与规则网络和随机网络截然不同的统计特性。</p>\n<span id=\"more\"></span>\n<p>本文采用的无标度网络生成模型是由 Barabási 和 Albert 于 1999 年提出的增长网络网络模型（BA 模型）。在该模型中，网络初始时具有 $m_0$ 个节点，两两互连。 之后每过一个时间单位增加一个新节点。新节点从当前网络中选择$m(m ≤ m_0)$ 个节点与之连接， 某节点 $v_i$ 被选中的概率 $p(v_i)$ 与其节点度 $d_i$ 的大小成正比，即<script type=\"math/tex\">p(v_i) = \\frac{d_i}{\\sum_j d_j}</script>经过 t 个时间单位后，网络中含有 $m_0+t$ 个节点，$m_0(m_0-1)/2+mt$条边。可以证明当 t 足够大时， 按此规律增长的网络的度分布为幂指数等于 3 的幂律分布。</p>\n<p>依据新节点的连接规律，建立节点度演化的动力学模型：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial d_i}{\\partial t} &= m \\frac{\\partial d_i}{\\sum_j d_j} \\\\\n&= m \\frac{d_i}{2\\left( \\frac{m_0(m_0-1)}{2} + mt\\right)} \\\\\n&= m \\frac{d_i}{\\left( m_0(m_0-1) + 2mt\\right)} \\\\\n&\\simeq \\frac{d_i}{2t}\n\\end{aligned}</script><p>其中最后一个等式在 $t$ 足够大时近似成立。 将节点 $i$ 加入网络的时间记为 $t_i$，<br>则有初始条件 $d_i(t_i) = m$。解得</p>\n<script type=\"math/tex; mode=display\">\nd_i = \\left\\{ \n\\begin{array}{ll}\n0, & t < t_i, \\\\\nm(\\frac{t}{t_i})^{0.5}, &t\\geq t_i.\n\\end{array}\n\\right.</script><p>在 $t$ 足够大， 对任意节点 $i$， 其度的大小满足</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(d_i(t) < d) &= P\\left( t_i > \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - P\\left(t_i \\leq \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - \\frac{m^2 t}{d^2(m_0 + t)} \\\\\n&\\simeq 1- \\frac{m^2}{d^2}\n\\end{aligned}</script><p>其中第三个等式成立的原因是加入节点的时间是等间隔的。上式正是网络节点度的概率分布函数， 可以求出节点度的概率密度函数 $p(d)$ 为</p>\n<script type=\"math/tex; mode=display\">\np(d) = \\frac{\\partial P(d_i(t) < d)}{\\partial d} = 2\\frac{m^2}{d^3}</script><p>可知所生成网络的幂律分布的指数为 3。</p>\n<p>下面的matlab程序模拟了BA网络的演化过程：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">scale_free</span><span class=\"params\">(N,m0,m)</span></span></span><br><span class=\"line\"><span class=\"comment\">%</span></span><br><span class=\"line\"><span class=\"comment\">%param  N: num of vertices 期望节点数</span></span><br><span class=\"line\"><span class=\"comment\">%param m0: num of initial vertices 初始边数</span></span><br><span class=\"line\"><span class=\"comment\">%param  m: num of vertices a new node try to connect 新节点连接的边数</span></span><br><span class=\"line\"><span class=\"comment\">%</span></span><br><span class=\"line\">tic;</span><br><span class=\"line\"></span><br><span class=\"line\">I = <span class=\"number\">2</span> ;    <span class=\"comment\">%生成的网络个数，只为统计需要</span></span><br><span class=\"line\"></span><br><span class=\"line\">realization_of_distribution = sparse( I , N ) ;</span><br><span class=\"line\"><span class=\"keyword\">for</span> J = <span class=\"number\">1</span> : I</span><br><span class=\"line\">    format long;</span><br><span class=\"line\"></span><br><span class=\"line\"> \t<span class=\"comment\">%初始化邻接矩阵，前m0个节点两两互连</span></span><br><span class=\"line\">    adjacent_matrix = sparse( m0 , m0 ) ;</span><br><span class=\"line\">    <span class=\"keyword\">parfor</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span> : m0</span><br><span class=\"line\">        <span class=\"keyword\">for</span> <span class=\"built_in\">j</span> = <span class=\"number\">1</span> : m0</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">j</span> ~= <span class=\"built_in\">i</span></span><br><span class=\"line\">                adjacent_matrix( <span class=\"built_in\">i</span> , <span class=\"built_in\">j</span> ) = <span class=\"number\">1</span> ;</span><br><span class=\"line\">            <span class=\"keyword\">end</span></span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    adjacent_matrix = sparse( adjacent_matrix ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">% 计算当前节点度分布</span></span><br><span class=\"line\">    node_degree = sparse( <span class=\"number\">1</span> , m0 ) ;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p = <span class=\"number\">1</span> : m0</span><br><span class=\"line\">        node_degree( p ) = sum( adjacent_matrix( <span class=\"number\">1</span> : m0 , p ) ) ;</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">% 开始演化</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> iteration = m0 + <span class=\"number\">1</span> : N</span><br><span class=\"line\">        total_degree = <span class=\"number\">2</span> * m * ( iteration - m0 <span class=\"number\">-1</span> ) + m0*(m0<span class=\"number\">-1</span>) ; <span class=\"comment\">% m*2</span></span><br><span class=\"line\">        degree_frequency = node_degree / total_degree ;</span><br><span class=\"line\">        cum_distribution = cumsum( degree_frequency ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">        choose = <span class=\"built_in\">zeros</span>( <span class=\"number\">1</span> , m ) ;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> new_edge = <span class=\"number\">1</span>:m</span><br><span class=\"line\">            r = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>) ;</span><br><span class=\"line\">            choose_edge = <span class=\"built_in\">find</span>( cum_distribution &gt;= r ,<span class=\"number\">1</span>) ;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> any(choose == choose_edge)</span><br><span class=\"line\">                r = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>) ;</span><br><span class=\"line\">                choose_edge = <span class=\"built_in\">find</span>(  cum_distribution &gt;= r,<span class=\"number\">1</span>) ;</span><br><span class=\"line\">            <span class=\"keyword\">end</span></span><br><span class=\"line\">            choose(new_edge) = choose_edge;</span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> k = <span class=\"number\">1</span> : m</span><br><span class=\"line\">            adjacent_matrix( iteration , choose(k) ) = <span class=\"number\">1</span> ;</span><br><span class=\"line\">            adjacent_matrix( choose(k) , iteration ) = <span class=\"number\">1</span> ;</span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> p = <span class=\"number\">1</span> : iteration</span><br><span class=\"line\">            node_degree(p) = sum( adjacent_matrix( <span class=\"number\">1</span> : iteration , p ) ) ;</span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">    number_of_nodes_with_equal_degree = <span class=\"built_in\">zeros</span>( <span class=\"number\">1</span> , N ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">parfor</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span> : N</span><br><span class=\"line\">        number_of_nodes_with_equal_degree(<span class=\"built_in\">i</span>) = <span class=\"built_in\">length</span>( <span class=\"built_in\">find</span>( node_degree == <span class=\"built_in\">i</span> ) ) ;</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    realization_of_distribution( J , : ) = number_of_nodes_with_equal_degree ;</span><br><span class=\"line\"></span><br><span class=\"line\">    save([<span class=\"string\">&#x27;adj_&#x27;</span>,num2str(J)],<span class=\"string\">&#x27;adjacent_matrix&#x27;</span>);</span><br><span class=\"line\"><span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">%&#123;</span></span><br><span class=\"line\"><span class=\"comment\">%plot degree distribution 在双对数坐标下画图</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">average = sum( realization_of_distribution )/ ( I * N );</span></span><br><span class=\"line\"><span class=\"comment\">loglog( 1:N , average , &#x27;*&#x27; )</span></span><br><span class=\"line\"><span class=\"comment\">axis([1 N 0.0000001 0.9])</span></span><br><span class=\"line\"><span class=\"comment\">hold on;</span></span><br><span class=\"line\"><span class=\"comment\">x = 1:N;</span></span><br><span class=\"line\"><span class=\"comment\">y = 2 * m^2 * x .^ ( -3 ) ;</span></span><br><span class=\"line\"><span class=\"comment\">loglog( x , y , &#x27;r&#x27; ) ;  %  p(k)=2*m^2*k^(-3)</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">%&#125;</span></span><br><span class=\"line\">toc;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">end</span></span><br></pre></td></tr></table></figure>\n<p>人工生成网络的概率质量函数（网络节点数 $N$ 分别为 50、 100、 200、 400）</p>\n<p><img src=\"degree_dist.png\" alt=\"生成网络的节点度分布\"></p>\n<p>图中直线为理论结果： $p(d)=2\\frac{m^2}{d^3}$。</p>\n<p><img src=\"/images/世界人民大团结万岁.gif\" alt=\"\"></p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"<p>1999 年 Barabási 和 Albert 提出了无标度网络模型（简称 BA 模型）。无标度网络的重要特征为： 无标度网络的节点度分布服从幂律分布。</p>\n<p>无标度网络的度分布 $p(d)$ 满足<script type=\"math/tex\">p(d)\\sim d^{-\\alpha}，</script>其中 $d$ 代表度的大小， $\\alpha$ 为度分布的幂律指数。 真实网络 $\\alpha$ 值一般介于 2~3之间。</p>\n<p>近年来越来越多的研究表明， 真实世界网络既不是规则网络， 也不是随机网络， 而是兼具小世界和无标度特性的复杂网络， 具有与规则网络和随机网络截然不同的统计特性。</p>","more":"<p>本文采用的无标度网络生成模型是由 Barabási 和 Albert 于 1999 年提出的增长网络网络模型（BA 模型）。在该模型中，网络初始时具有 $m_0$ 个节点，两两互连。 之后每过一个时间单位增加一个新节点。新节点从当前网络中选择$m(m ≤ m_0)$ 个节点与之连接， 某节点 $v_i$ 被选中的概率 $p(v_i)$ 与其节点度 $d_i$ 的大小成正比，即<script type=\"math/tex\">p(v_i) = \\frac{d_i}{\\sum_j d_j}</script>经过 t 个时间单位后，网络中含有 $m_0+t$ 个节点，$m_0(m_0-1)/2+mt$条边。可以证明当 t 足够大时， 按此规律增长的网络的度分布为幂指数等于 3 的幂律分布。</p>\n<p>依据新节点的连接规律，建立节点度演化的动力学模型：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial d_i}{\\partial t} &= m \\frac{\\partial d_i}{\\sum_j d_j} \\\\\n&= m \\frac{d_i}{2\\left( \\frac{m_0(m_0-1)}{2} + mt\\right)} \\\\\n&= m \\frac{d_i}{\\left( m_0(m_0-1) + 2mt\\right)} \\\\\n&\\simeq \\frac{d_i}{2t}\n\\end{aligned}</script><p>其中最后一个等式在 $t$ 足够大时近似成立。 将节点 $i$ 加入网络的时间记为 $t_i$，<br>则有初始条件 $d_i(t_i) = m$。解得</p>\n<script type=\"math/tex; mode=display\">\nd_i = \\left\\{ \n\\begin{array}{ll}\n0, & t < t_i, \\\\\nm(\\frac{t}{t_i})^{0.5}, &t\\geq t_i.\n\\end{array}\n\\right.</script><p>在 $t$ 足够大， 对任意节点 $i$， 其度的大小满足</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(d_i(t) < d) &= P\\left( t_i > \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - P\\left(t_i \\leq \\frac{m^2 t}{d^2}\\right) \\\\\n&= 1 - \\frac{m^2 t}{d^2(m_0 + t)} \\\\\n&\\simeq 1- \\frac{m^2}{d^2}\n\\end{aligned}</script><p>其中第三个等式成立的原因是加入节点的时间是等间隔的。上式正是网络节点度的概率分布函数， 可以求出节点度的概率密度函数 $p(d)$ 为</p>\n<script type=\"math/tex; mode=display\">\np(d) = \\frac{\\partial P(d_i(t) < d)}{\\partial d} = 2\\frac{m^2}{d^3}</script><p>可知所生成网络的幂律分布的指数为 3。</p>\n<p>下面的matlab程序模拟了BA网络的演化过程：</p>\n<figure class=\"highlight matlab\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">scale_free</span><span class=\"params\">(N,m0,m)</span></span></span><br><span class=\"line\"><span class=\"comment\">%</span></span><br><span class=\"line\"><span class=\"comment\">%param  N: num of vertices 期望节点数</span></span><br><span class=\"line\"><span class=\"comment\">%param m0: num of initial vertices 初始边数</span></span><br><span class=\"line\"><span class=\"comment\">%param  m: num of vertices a new node try to connect 新节点连接的边数</span></span><br><span class=\"line\"><span class=\"comment\">%</span></span><br><span class=\"line\">tic;</span><br><span class=\"line\"></span><br><span class=\"line\">I = <span class=\"number\">2</span> ;    <span class=\"comment\">%生成的网络个数，只为统计需要</span></span><br><span class=\"line\"></span><br><span class=\"line\">realization_of_distribution = sparse( I , N ) ;</span><br><span class=\"line\"><span class=\"keyword\">for</span> J = <span class=\"number\">1</span> : I</span><br><span class=\"line\">    format long;</span><br><span class=\"line\"></span><br><span class=\"line\"> \t<span class=\"comment\">%初始化邻接矩阵，前m0个节点两两互连</span></span><br><span class=\"line\">    adjacent_matrix = sparse( m0 , m0 ) ;</span><br><span class=\"line\">    <span class=\"keyword\">parfor</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span> : m0</span><br><span class=\"line\">        <span class=\"keyword\">for</span> <span class=\"built_in\">j</span> = <span class=\"number\">1</span> : m0</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">j</span> ~= <span class=\"built_in\">i</span></span><br><span class=\"line\">                adjacent_matrix( <span class=\"built_in\">i</span> , <span class=\"built_in\">j</span> ) = <span class=\"number\">1</span> ;</span><br><span class=\"line\">            <span class=\"keyword\">end</span></span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    adjacent_matrix = sparse( adjacent_matrix ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">% 计算当前节点度分布</span></span><br><span class=\"line\">    node_degree = sparse( <span class=\"number\">1</span> , m0 ) ;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p = <span class=\"number\">1</span> : m0</span><br><span class=\"line\">        node_degree( p ) = sum( adjacent_matrix( <span class=\"number\">1</span> : m0 , p ) ) ;</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">% 开始演化</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> iteration = m0 + <span class=\"number\">1</span> : N</span><br><span class=\"line\">        total_degree = <span class=\"number\">2</span> * m * ( iteration - m0 <span class=\"number\">-1</span> ) + m0*(m0<span class=\"number\">-1</span>) ; <span class=\"comment\">% m*2</span></span><br><span class=\"line\">        degree_frequency = node_degree / total_degree ;</span><br><span class=\"line\">        cum_distribution = cumsum( degree_frequency ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">        choose = <span class=\"built_in\">zeros</span>( <span class=\"number\">1</span> , m ) ;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> new_edge = <span class=\"number\">1</span>:m</span><br><span class=\"line\">            r = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>) ;</span><br><span class=\"line\">            choose_edge = <span class=\"built_in\">find</span>( cum_distribution &gt;= r ,<span class=\"number\">1</span>) ;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> any(choose == choose_edge)</span><br><span class=\"line\">                r = <span class=\"built_in\">rand</span>(<span class=\"number\">1</span>) ;</span><br><span class=\"line\">                choose_edge = <span class=\"built_in\">find</span>(  cum_distribution &gt;= r,<span class=\"number\">1</span>) ;</span><br><span class=\"line\">            <span class=\"keyword\">end</span></span><br><span class=\"line\">            choose(new_edge) = choose_edge;</span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> k = <span class=\"number\">1</span> : m</span><br><span class=\"line\">            adjacent_matrix( iteration , choose(k) ) = <span class=\"number\">1</span> ;</span><br><span class=\"line\">            adjacent_matrix( choose(k) , iteration ) = <span class=\"number\">1</span> ;</span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> p = <span class=\"number\">1</span> : iteration</span><br><span class=\"line\">            node_degree(p) = sum( adjacent_matrix( <span class=\"number\">1</span> : iteration , p ) ) ;</span><br><span class=\"line\">        <span class=\"keyword\">end</span></span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\">    number_of_nodes_with_equal_degree = <span class=\"built_in\">zeros</span>( <span class=\"number\">1</span> , N ) ;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">parfor</span> <span class=\"built_in\">i</span> = <span class=\"number\">1</span> : N</span><br><span class=\"line\">        number_of_nodes_with_equal_degree(<span class=\"built_in\">i</span>) = <span class=\"built_in\">length</span>( <span class=\"built_in\">find</span>( node_degree == <span class=\"built_in\">i</span> ) ) ;</span><br><span class=\"line\">    <span class=\"keyword\">end</span></span><br><span class=\"line\">    realization_of_distribution( J , : ) = number_of_nodes_with_equal_degree ;</span><br><span class=\"line\"></span><br><span class=\"line\">    save([<span class=\"string\">&#x27;adj_&#x27;</span>,num2str(J)],<span class=\"string\">&#x27;adjacent_matrix&#x27;</span>);</span><br><span class=\"line\"><span class=\"keyword\">end</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">%&#123;</span></span><br><span class=\"line\"><span class=\"comment\">%plot degree distribution 在双对数坐标下画图</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">average = sum( realization_of_distribution )/ ( I * N );</span></span><br><span class=\"line\"><span class=\"comment\">loglog( 1:N , average , &#x27;*&#x27; )</span></span><br><span class=\"line\"><span class=\"comment\">axis([1 N 0.0000001 0.9])</span></span><br><span class=\"line\"><span class=\"comment\">hold on;</span></span><br><span class=\"line\"><span class=\"comment\">x = 1:N;</span></span><br><span class=\"line\"><span class=\"comment\">y = 2 * m^2 * x .^ ( -3 ) ;</span></span><br><span class=\"line\"><span class=\"comment\">loglog( x , y , &#x27;r&#x27; ) ;  %  p(k)=2*m^2*k^(-3)</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">%&#125;</span></span><br><span class=\"line\">toc;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">end</span></span><br></pre></td></tr></table></figure>\n<p>人工生成网络的概率质量函数（网络节点数 $N$ 分别为 50、 100、 200、 400）</p>\n<p><img src=\"degree_dist.png\" alt=\"生成网络的节点度分布\"></p>\n<p>图中直线为理论结果： $p(d)=2\\frac{m^2}{d^3}$。</p>\n<p><img src=\"/images/世界人民大团结万岁.gif\" alt=\"\"></p>"},{"title":"构造极限环","date":"2021-03-23T14:54:19.000Z","mathjax":true,"_content":"\n在极坐标下容易想到，使半径收敛到常数 $R$ 即可\n$$\n\\begin{aligned}\n\\dot{r} &= -r(r^2-R^2)\\\\\n\\dot{\\theta} &= \\omega\n\\end{aligned}\n$$\n\n其中 $R,\\omega$ 为极限环的半径和角速度\n\n<!--more-->\n\n转化成直角坐标：\n$$\n\\begin{aligned}\n\\dot{x} &= (r\\cos\\theta)' = \\dot{r}\\cos\\theta - r\\sin\\theta\\dot{\\theta}\\\\\n&=-r(r^2-R^2)\\cos\\theta- r \\omega \\sin\\theta \\\\\n&= -x(x^2+y^2-R^2) - \\omega y\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\dot{y} &= (r\\sin\\theta)' = \\dot{r}\\sin\\theta + r\\cos\\theta\\dot{\\theta}\\\\\n&= -r(r^2-R^2)\\sin\\theta + r \\omega \\cos\\theta \\\\\n&= -y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}\n$$\n\n即\n$$\n\\begin{aligned}\n\\dot{x} &=  -x(x^2+y^2-R^2) - \\omega y\\\\\n\\dot{y} &= - y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}\n$$\n\n不妨令 $R=1,\\omega=1$，用 matlab 画相图如下：\n```\nclc;clear;close;\n[x,y]=meshgrid(linspace(-3,3));\nh=streamslice(x,y, -y-x.*(x.^2+y.^2-1), x -y.*(x.^2+y.^2-1));\ntitle('Limit Circle')\nxlabel('x');ylabel('y');\nxlim([-3,3]);ylim([-3,3]);\nset(h,'Color','k')\naxis equal\nhold on\ntheta=0:pi/30:2*pi;\nx1=cos(theta);y1=sin(theta);\nplot(x1,y1,'r--')\n```\n\n![极限环](pic1.png)\n```\nclc;clear;close;\n[x,y]=meshgrid(-1.5:0.2:1.5,-1.5:0.2:1.5);\nu=-y-x.*(x.^2+y.^2-1);\nv=x-y.*(x.^2+y.^2-1); \nhadl=quiver(x,y,u,v)\ntitle('Limit Circle')\nset(hadl,'Color','k')\naxis equal\nxlabel('x');ylabel('y');\nxlim([-1.5,1.5]);ylim([-1.5,1.5]);\nhold on\ntheta=0:pi/30:2*pi;\nx1=cos(theta);y1=sin(theta);\nplot(x1,y1,'r--')\n```\n![极限环](pic2.png)\n","source":"_posts/构造极限环.md","raw":"---\ntitle: 构造极限环\ndate: 2021-03-23 22:54:19\ntags: [极限环, matlab]\ncategories: 极限环\nmathjax: true\n---\n\n在极坐标下容易想到，使半径收敛到常数 $R$ 即可\n$$\n\\begin{aligned}\n\\dot{r} &= -r(r^2-R^2)\\\\\n\\dot{\\theta} &= \\omega\n\\end{aligned}\n$$\n\n其中 $R,\\omega$ 为极限环的半径和角速度\n\n<!--more-->\n\n转化成直角坐标：\n$$\n\\begin{aligned}\n\\dot{x} &= (r\\cos\\theta)' = \\dot{r}\\cos\\theta - r\\sin\\theta\\dot{\\theta}\\\\\n&=-r(r^2-R^2)\\cos\\theta- r \\omega \\sin\\theta \\\\\n&= -x(x^2+y^2-R^2) - \\omega y\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\dot{y} &= (r\\sin\\theta)' = \\dot{r}\\sin\\theta + r\\cos\\theta\\dot{\\theta}\\\\\n&= -r(r^2-R^2)\\sin\\theta + r \\omega \\cos\\theta \\\\\n&= -y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}\n$$\n\n即\n$$\n\\begin{aligned}\n\\dot{x} &=  -x(x^2+y^2-R^2) - \\omega y\\\\\n\\dot{y} &= - y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}\n$$\n\n不妨令 $R=1,\\omega=1$，用 matlab 画相图如下：\n```\nclc;clear;close;\n[x,y]=meshgrid(linspace(-3,3));\nh=streamslice(x,y, -y-x.*(x.^2+y.^2-1), x -y.*(x.^2+y.^2-1));\ntitle('Limit Circle')\nxlabel('x');ylabel('y');\nxlim([-3,3]);ylim([-3,3]);\nset(h,'Color','k')\naxis equal\nhold on\ntheta=0:pi/30:2*pi;\nx1=cos(theta);y1=sin(theta);\nplot(x1,y1,'r--')\n```\n\n![极限环](pic1.png)\n```\nclc;clear;close;\n[x,y]=meshgrid(-1.5:0.2:1.5,-1.5:0.2:1.5);\nu=-y-x.*(x.^2+y.^2-1);\nv=x-y.*(x.^2+y.^2-1); \nhadl=quiver(x,y,u,v)\ntitle('Limit Circle')\nset(hadl,'Color','k')\naxis equal\nxlabel('x');ylabel('y');\nxlim([-1.5,1.5]);ylim([-1.5,1.5]);\nhold on\ntheta=0:pi/30:2*pi;\nx1=cos(theta);y1=sin(theta);\nplot(x1,y1,'r--')\n```\n![极限环](pic2.png)\n","slug":"构造极限环","published":1,"updated":"2021-03-23T14:59:48.664Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmm5gsqe0007kzj84jvv3pas","content":"<p>在极坐标下容易想到，使半径收敛到常数 $R$ 即可</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{r} &= -r(r^2-R^2)\\\\\n\\dot{\\theta} &= \\omega\n\\end{aligned}</script><p>其中 $R,\\omega$ 为极限环的半径和角速度</p>\n<span id=\"more\"></span>\n<p>转化成直角坐标：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{x} &= (r\\cos\\theta)' = \\dot{r}\\cos\\theta - r\\sin\\theta\\dot{\\theta}\\\\\n&=-r(r^2-R^2)\\cos\\theta- r \\omega \\sin\\theta \\\\\n&= -x(x^2+y^2-R^2) - \\omega y\n\\end{aligned}</script><script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{y} &= (r\\sin\\theta)' = \\dot{r}\\sin\\theta + r\\cos\\theta\\dot{\\theta}\\\\\n&= -r(r^2-R^2)\\sin\\theta + r \\omega \\cos\\theta \\\\\n&= -y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}</script><p>即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{x} &=  -x(x^2+y^2-R^2) - \\omega y\\\\\n\\dot{y} &= - y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}</script><p>不妨令 $R=1,\\omega=1$，用 matlab 画相图如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">clc;clear;close;</span><br><span class=\"line\">[x,y]&#x3D;meshgrid(linspace(-3,3));</span><br><span class=\"line\">h&#x3D;streamslice(x,y, -y-x.*(x.^2+y.^2-1), x -y.*(x.^2+y.^2-1));</span><br><span class=\"line\">title(&#39;Limit Circle&#39;)</span><br><span class=\"line\">xlabel(&#39;x&#39;);ylabel(&#39;y&#39;);</span><br><span class=\"line\">xlim([-3,3]);ylim([-3,3]);</span><br><span class=\"line\">set(h,&#39;Color&#39;,&#39;k&#39;)</span><br><span class=\"line\">axis equal</span><br><span class=\"line\">hold on</span><br><span class=\"line\">theta&#x3D;0:pi&#x2F;30:2*pi;</span><br><span class=\"line\">x1&#x3D;cos(theta);y1&#x3D;sin(theta);</span><br><span class=\"line\">plot(x1,y1,&#39;r--&#39;)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"pic1.png\" alt=\"极限环\"><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">clc;clear;close;</span><br><span class=\"line\">[x,y]&#x3D;meshgrid(-1.5:0.2:1.5,-1.5:0.2:1.5);</span><br><span class=\"line\">u&#x3D;-y-x.*(x.^2+y.^2-1);</span><br><span class=\"line\">v&#x3D;x-y.*(x.^2+y.^2-1); </span><br><span class=\"line\">hadl&#x3D;quiver(x,y,u,v)</span><br><span class=\"line\">title(&#39;Limit Circle&#39;)</span><br><span class=\"line\">set(hadl,&#39;Color&#39;,&#39;k&#39;)</span><br><span class=\"line\">axis equal</span><br><span class=\"line\">xlabel(&#39;x&#39;);ylabel(&#39;y&#39;);</span><br><span class=\"line\">xlim([-1.5,1.5]);ylim([-1.5,1.5]);</span><br><span class=\"line\">hold on</span><br><span class=\"line\">theta&#x3D;0:pi&#x2F;30:2*pi;</span><br><span class=\"line\">x1&#x3D;cos(theta);y1&#x3D;sin(theta);</span><br><span class=\"line\">plot(x1,y1,&#39;r--&#39;)</span><br></pre></td></tr></table></figure><br><img src=\"pic2.png\" alt=\"极限环\"></p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"<p>在极坐标下容易想到，使半径收敛到常数 $R$ 即可</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{r} &= -r(r^2-R^2)\\\\\n\\dot{\\theta} &= \\omega\n\\end{aligned}</script><p>其中 $R,\\omega$ 为极限环的半径和角速度</p>","more":"<p>转化成直角坐标：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{x} &= (r\\cos\\theta)' = \\dot{r}\\cos\\theta - r\\sin\\theta\\dot{\\theta}\\\\\n&=-r(r^2-R^2)\\cos\\theta- r \\omega \\sin\\theta \\\\\n&= -x(x^2+y^2-R^2) - \\omega y\n\\end{aligned}</script><script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{y} &= (r\\sin\\theta)' = \\dot{r}\\sin\\theta + r\\cos\\theta\\dot{\\theta}\\\\\n&= -r(r^2-R^2)\\sin\\theta + r \\omega \\cos\\theta \\\\\n&= -y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}</script><p>即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\dot{x} &=  -x(x^2+y^2-R^2) - \\omega y\\\\\n\\dot{y} &= - y(x^2+y^2-R^2) + \\omega x\n\\end{aligned}</script><p>不妨令 $R=1,\\omega=1$，用 matlab 画相图如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">clc;clear;close;</span><br><span class=\"line\">[x,y]&#x3D;meshgrid(linspace(-3,3));</span><br><span class=\"line\">h&#x3D;streamslice(x,y, -y-x.*(x.^2+y.^2-1), x -y.*(x.^2+y.^2-1));</span><br><span class=\"line\">title(&#39;Limit Circle&#39;)</span><br><span class=\"line\">xlabel(&#39;x&#39;);ylabel(&#39;y&#39;);</span><br><span class=\"line\">xlim([-3,3]);ylim([-3,3]);</span><br><span class=\"line\">set(h,&#39;Color&#39;,&#39;k&#39;)</span><br><span class=\"line\">axis equal</span><br><span class=\"line\">hold on</span><br><span class=\"line\">theta&#x3D;0:pi&#x2F;30:2*pi;</span><br><span class=\"line\">x1&#x3D;cos(theta);y1&#x3D;sin(theta);</span><br><span class=\"line\">plot(x1,y1,&#39;r--&#39;)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"pic1.png\" alt=\"极限环\"><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">clc;clear;close;</span><br><span class=\"line\">[x,y]&#x3D;meshgrid(-1.5:0.2:1.5,-1.5:0.2:1.5);</span><br><span class=\"line\">u&#x3D;-y-x.*(x.^2+y.^2-1);</span><br><span class=\"line\">v&#x3D;x-y.*(x.^2+y.^2-1); </span><br><span class=\"line\">hadl&#x3D;quiver(x,y,u,v)</span><br><span class=\"line\">title(&#39;Limit Circle&#39;)</span><br><span class=\"line\">set(hadl,&#39;Color&#39;,&#39;k&#39;)</span><br><span class=\"line\">axis equal</span><br><span class=\"line\">xlabel(&#39;x&#39;);ylabel(&#39;y&#39;);</span><br><span class=\"line\">xlim([-1.5,1.5]);ylim([-1.5,1.5]);</span><br><span class=\"line\">hold on</span><br><span class=\"line\">theta&#x3D;0:pi&#x2F;30:2*pi;</span><br><span class=\"line\">x1&#x3D;cos(theta);y1&#x3D;sin(theta);</span><br><span class=\"line\">plot(x1,y1,&#39;r--&#39;)</span><br></pre></td></tr></table></figure><br><img src=\"pic2.png\" alt=\"极限环\"></p>"},{"title":"计算一阶导数的四阶中心差分格式","date":"2021-03-23T15:01:37.000Z","mathjax":true,"_content":"\n![四阶中心差分](四阶中心差分.png)\n\n原理：利用指定点 $y_t$ 周围的四个点$(y_{t-2}, y_{t-1}, y_{t+2},y_{t+2})$构造 **拉格朗日插值曲线**\n\n<!--more-->\n$$\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(x_{t-2}-x_{t-1})(x_{t-2}-x_{t+1})(x_{t-2}-x_{t+2})} \\\\\n\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(x_{t-1}-x_{t-2})(x_{t-1}-x_{t+1})(x_{t-1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(x_{t+1}-x_{t-2})(x_{t+1}-x_{t-1})(x_{t+1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(x_{t+2}-x_{t-2})(x_{t+2}-x_{t-1})(x_{t+2}-x_{t+1})}\n\\end{aligned}\n$$\n\n分母是可以直接写出来的，记 $h \\equiv x_t - x_{t-1}$ 为自变量间隔：\n$$\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(-h)(-3h)(-4h)} \\\\\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(h)(-2h)(-3h)} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(3h)(2h)(-h)} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(4h)(3h)(h)} \\\\\\\\\n=&[  -y_{t-2}(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})\\\\\n& + 2y_{t-1}(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})\\\\\n& -2y_{t+1}(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})\\\\\n& + y_{t+2}(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})  ]\\frac{1}{12h^3} \n\\end{aligned}\n$$\n\n接下来就是求该三次函数在 $x_t$ 处的导数 $y'(x_{t})$\n$$\n\\begin{aligned}\ny'(x) =& \\{  -y_{t-2}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x-x_{t-1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x-x_{t-1})(x-x_{t+1})+(x-x_{t-2})(x-x_{t+1})+(x-x_{t-2})(x-x_{t-1})] \\}\\frac{1}{12h^3} \n\\end{aligned}\n$$\n所以\n$$\n\\begin{aligned}\ny'(x_t) =& \\{  -y_{t-2}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x_t-x_{t-1})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t-1})] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  \\{  -y_{t-2}[(-h)(-2h)+(h)(-2h)+(h)(-h)]\\\\\n\\\\\n& + 2y_{t-1}[(-h)(-2h)+(2h)(-2h)+(2h)(-h)]\\\\\n\\\\\n& -2y_{t+1}[(h)(-2h)+(2h)(-2h)+(2h)(h)]\\\\\n\\\\\n& + y_{t+2}[(h)(-h)+(2h)(-h)+(2h)(h)] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  (  y_{t-2}  -8y_{t-1} +8y_{t+1}- y_{t+2} )\\frac{1}{12h} \\\\\n\\end{aligned}\n$$\n\n这就是计算一阶导数的四阶中心差分格式！\n","source":"_posts/计算一阶导数的四阶中心差分格式.md","raw":"---\ntitle: 计算一阶导数的四阶中心差分格式\ndate: 2021-03-23 23:01:37\ntags: [导数, 中心差分]\ncategories: 计算方法\nmathjax: true\n---\n\n![四阶中心差分](四阶中心差分.png)\n\n原理：利用指定点 $y_t$ 周围的四个点$(y_{t-2}, y_{t-1}, y_{t+2},y_{t+2})$构造 **拉格朗日插值曲线**\n\n<!--more-->\n$$\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(x_{t-2}-x_{t-1})(x_{t-2}-x_{t+1})(x_{t-2}-x_{t+2})} \\\\\n\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(x_{t-1}-x_{t-2})(x_{t-1}-x_{t+1})(x_{t-1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(x_{t+1}-x_{t-2})(x_{t+1}-x_{t-1})(x_{t+1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(x_{t+2}-x_{t-2})(x_{t+2}-x_{t-1})(x_{t+2}-x_{t+1})}\n\\end{aligned}\n$$\n\n分母是可以直接写出来的，记 $h \\equiv x_t - x_{t-1}$ 为自变量间隔：\n$$\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(-h)(-3h)(-4h)} \\\\\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(h)(-2h)(-3h)} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(3h)(2h)(-h)} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(4h)(3h)(h)} \\\\\\\\\n=&[  -y_{t-2}(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})\\\\\n& + 2y_{t-1}(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})\\\\\n& -2y_{t+1}(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})\\\\\n& + y_{t+2}(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})  ]\\frac{1}{12h^3} \n\\end{aligned}\n$$\n\n接下来就是求该三次函数在 $x_t$ 处的导数 $y'(x_{t})$\n$$\n\\begin{aligned}\ny'(x) =& \\{  -y_{t-2}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x-x_{t-1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x-x_{t-1})(x-x_{t+1})+(x-x_{t-2})(x-x_{t+1})+(x-x_{t-2})(x-x_{t-1})] \\}\\frac{1}{12h^3} \n\\end{aligned}\n$$\n所以\n$$\n\\begin{aligned}\ny'(x_t) =& \\{  -y_{t-2}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x_t-x_{t-1})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t-1})] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  \\{  -y_{t-2}[(-h)(-2h)+(h)(-2h)+(h)(-h)]\\\\\n\\\\\n& + 2y_{t-1}[(-h)(-2h)+(2h)(-2h)+(2h)(-h)]\\\\\n\\\\\n& -2y_{t+1}[(h)(-2h)+(2h)(-2h)+(2h)(h)]\\\\\n\\\\\n& + y_{t+2}[(h)(-h)+(2h)(-h)+(2h)(h)] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  (  y_{t-2}  -8y_{t-1} +8y_{t+1}- y_{t+2} )\\frac{1}{12h} \\\\\n\\end{aligned}\n$$\n\n这就是计算一阶导数的四阶中心差分格式！\n","slug":"计算一阶导数的四阶中心差分格式","published":1,"updated":"2021-03-23T15:05:08.431Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmm5gsqf0008kzj82mq6g3p6","content":"<p><img src=\"四阶中心差分.png\" alt=\"四阶中心差分\"></p>\n<p>原理：利用指定点 $y_t$ 周围的四个点$(y_{t-2}, y_{t-1}, y_{t+2},y_{t+2})$构造 <strong>拉格朗日插值曲线</strong></p>\n<span id=\"more\"></span>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(x_{t-2}-x_{t-1})(x_{t-2}-x_{t+1})(x_{t-2}-x_{t+2})} \\\\\n\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(x_{t-1}-x_{t-2})(x_{t-1}-x_{t+1})(x_{t-1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(x_{t+1}-x_{t-2})(x_{t+1}-x_{t-1})(x_{t+1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(x_{t+2}-x_{t-2})(x_{t+2}-x_{t-1})(x_{t+2}-x_{t+1})}\n\\end{aligned}</script><p>分母是可以直接写出来的，记 $h \\equiv x_t - x_{t-1}$ 为自变量间隔：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(-h)(-3h)(-4h)} \\\\\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(h)(-2h)(-3h)} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(3h)(2h)(-h)} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(4h)(3h)(h)} \\\\\\\\\n=&[  -y_{t-2}(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})\\\\\n& + 2y_{t-1}(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})\\\\\n& -2y_{t+1}(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})\\\\\n& + y_{t+2}(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})  ]\\frac{1}{12h^3} \n\\end{aligned}</script><p>接下来就是求该三次函数在 $x_t$ 处的导数 $y’(x_{t})$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny'(x) =& \\{  -y_{t-2}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x-x_{t-1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x-x_{t-1})(x-x_{t+1})+(x-x_{t-2})(x-x_{t+1})+(x-x_{t-2})(x-x_{t-1})] \\}\\frac{1}{12h^3} \n\\end{aligned}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny'(x_t) =& \\{  -y_{t-2}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x_t-x_{t-1})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t-1})] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  \\{  -y_{t-2}[(-h)(-2h)+(h)(-2h)+(h)(-h)]\\\\\n\\\\\n& + 2y_{t-1}[(-h)(-2h)+(2h)(-2h)+(2h)(-h)]\\\\\n\\\\\n& -2y_{t+1}[(h)(-2h)+(2h)(-2h)+(2h)(h)]\\\\\n\\\\\n& + y_{t+2}[(h)(-h)+(2h)(-h)+(2h)(h)] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  (  y_{t-2}  -8y_{t-1} +8y_{t+1}- y_{t+2} )\\frac{1}{12h} \\\\\n\\end{aligned}</script><p>这就是计算一阶导数的四阶中心差分格式！</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/bg.jpeg\");\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n  background-size: cover;\n}\n.sidebar {\n  transition-duration: 0.4s;\n  opacity: 0.8;\n}\n.posts-expand .post-title-link {\n  color: #222;\n}\n.post-block {\n  background: #fff;\n  border-radius: initial;\n  box-shadow: 0 2px 2px 0 rgba(0,0,0,0.12), 0 3px 1px -2px rgba(0,0,0,0.06), 0 1px 5px 0 rgba(0,0,0,0.12);\n  padding: 40px;\n}\n"}},"excerpt":"<p><img src=\"四阶中心差分.png\" alt=\"四阶中心差分\"></p>\n<p>原理：利用指定点 $y_t$ 周围的四个点$(y_{t-2}, y_{t-1}, y_{t+2},y_{t+2})$构造 <strong>拉格朗日插值曲线</strong></p>","more":"<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(x_{t-2}-x_{t-1})(x_{t-2}-x_{t+1})(x_{t-2}-x_{t+2})} \\\\\n\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(x_{t-1}-x_{t-2})(x_{t-1}-x_{t+1})(x_{t-1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(x_{t+1}-x_{t-2})(x_{t+1}-x_{t-1})(x_{t+1}-x_{t+2})} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(x_{t+2}-x_{t-2})(x_{t+2}-x_{t-1})(x_{t+2}-x_{t+1})}\n\\end{aligned}</script><p>分母是可以直接写出来的，记 $h \\equiv x_t - x_{t-1}$ 为自变量间隔：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny(x) =& y_{t-2}\\frac{(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})}{(-h)(-3h)(-4h)} \\\\\\\\\n&+y_{t-1}\\frac{(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})}{(h)(-2h)(-3h)} \\\\\n\\\\\n&+y_{t+1}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})}{(3h)(2h)(-h)} \\\\\n\\\\\n&+y_{t+2}\\frac{(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})}{(4h)(3h)(h)} \\\\\\\\\n=&[  -y_{t-2}(x-x_{t-1})(x-x_{t+1})(x-x_{t+2})\\\\\n& + 2y_{t-1}(x-x_{t-2})(x-x_{t+1})(x-x_{t+2})\\\\\n& -2y_{t+1}(x-x_{t-2})(x-x_{t-1})(x-x_{t+2})\\\\\n& + y_{t+2}(x-x_{t-2})(x-x_{t-1})(x-x_{t+1})  ]\\frac{1}{12h^3} \n\\end{aligned}</script><p>接下来就是求该三次函数在 $x_t$ 处的导数 $y’(x_{t})$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny'(x) =& \\{  -y_{t-2}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+2})+(x-x_{t-1})(x-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x-x_{t+1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x-x_{t-1})(x-x_{t+2})+(x-x_{t-2})(x-x_{t+2})+(x-x_{t-2})(x-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x-x_{t-1})(x-x_{t+1})+(x-x_{t-2})(x-x_{t+1})+(x-x_{t-2})(x-x_{t-1})] \\}\\frac{1}{12h^3} \n\\end{aligned}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\ny'(x_t) =& \\{  -y_{t-2}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-1})(x_t-x_{t+1})]\\\\\n\\\\\n& + 2y_{t-1}[(x_t-x_{t+1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+1})]\\\\\n\\\\\n& -2y_{t+1}[(x_t-x_{t-1})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t+2})+(x_t-x_{t-2})(x_t-x_{t-1})]\\\\\n\\\\\n& + y_{t+2}[(x_t-x_{t-1})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t+1})+(x_t-x_{t-2})(x_t-x_{t-1})] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  \\{  -y_{t-2}[(-h)(-2h)+(h)(-2h)+(h)(-h)]\\\\\n\\\\\n& + 2y_{t-1}[(-h)(-2h)+(2h)(-2h)+(2h)(-h)]\\\\\n\\\\\n& -2y_{t+1}[(h)(-2h)+(2h)(-2h)+(2h)(h)]\\\\\n\\\\\n& + y_{t+2}[(h)(-h)+(2h)(-h)+(2h)(h)] \\}\\frac{1}{12h^3} \\\\\n\\\\\n=&  (  y_{t-2}  -8y_{t-1} +8y_{t+1}- y_{t+2} )\\frac{1}{12h} \\\\\n\\end{aligned}</script><p>这就是计算一阶导数的四阶中心差分格式！</p>"}],"PostAsset":[{"_id":"source/_posts/BPDC/RNN.png","slug":"RNN.png","post":"ckmm5gsq90001kzj8fxizhdb4","modified":0,"renderable":0},{"_id":"source/_posts/无标度网络的生成模型/degree_dist.png","slug":"degree_dist.png","post":"ckmm5gsqb0003kzj81ujf4rm9","modified":0,"renderable":0},{"_id":"source/_posts/构造极限环/pic1.png","slug":"pic1.png","post":"ckmm5gsqe0007kzj84jvv3pas","modified":0,"renderable":0},{"_id":"source/_posts/构造极限环/pic2.png","slug":"pic2.png","post":"ckmm5gsqe0007kzj84jvv3pas","modified":0,"renderable":0},{"_id":"source/_posts/计算一阶导数的四阶中心差分格式/四阶中心差分.png","slug":"四阶中心差分.png","post":"ckmm5gsqf0008kzj82mq6g3p6","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckmm5gsq90001kzj8fxizhdb4","category_id":"ckmm5gsqd0004kzj8bwld7lzt","_id":"ckmm5gsqh000bkzj86k5o2i8s"},{"post_id":"ckmm5gsqb0003kzj81ujf4rm9","category_id":"ckmm5gsqg0009kzj8bgeh82ea","_id":"ckmm5gsqh000ekzj8fdlnfvde"},{"post_id":"ckmm5gsqe0007kzj84jvv3pas","category_id":"ckmm5gsqh000ckzj8dl5o7xjr","_id":"ckmm5gsqi000hkzj84qklbnua"},{"post_id":"ckmm5gsqf0008kzj82mq6g3p6","category_id":"ckmm5gsqh000fkzj8e0zxd64c","_id":"ckmm5gsqi000lkzj865qu5e57"}],"PostTag":[{"post_id":"ckmm5gsq90001kzj8fxizhdb4","tag_id":"ckmm5gsqe0005kzj8eebxg0a5","_id":"ckmm5gsqi000jkzj89flveyb2"},{"post_id":"ckmm5gsq90001kzj8fxizhdb4","tag_id":"ckmm5gsqg000akzj84kf031np","_id":"ckmm5gsqi000kkzj87y4e995z"},{"post_id":"ckmm5gsq90001kzj8fxizhdb4","tag_id":"ckmm5gsqh000dkzj83rz7a1rj","_id":"ckmm5gsqi000nkzj8e7x2ae67"},{"post_id":"ckmm5gsq90001kzj8fxizhdb4","tag_id":"ckmm5gsqh000gkzj82kq4570m","_id":"ckmm5gsqi000okzj84y6g4v8q"},{"post_id":"ckmm5gsqb0003kzj81ujf4rm9","tag_id":"ckmm5gsqi000ikzj846y7crf8","_id":"ckmm5gsqj000qkzj8ck8x0plp"},{"post_id":"ckmm5gsqb0003kzj81ujf4rm9","tag_id":"ckmm5gsqi000mkzj858ls0phz","_id":"ckmm5gsqj000rkzj8ba0fho1h"},{"post_id":"ckmm5gsqe0007kzj84jvv3pas","tag_id":"ckmm5gsqi000pkzj8fmdm2hdm","_id":"ckmm5gsqk000ukzj89bmc48dn"},{"post_id":"ckmm5gsqe0007kzj84jvv3pas","tag_id":"ckmm5gsqj000skzj84313bnpt","_id":"ckmm5gsqk000vkzj8gv4o2xuq"},{"post_id":"ckmm5gsqf0008kzj82mq6g3p6","tag_id":"ckmm5gsqk000tkzj89ptg86sy","_id":"ckmm5gsqk000xkzj85iuy08s3"},{"post_id":"ckmm5gsqf0008kzj82mq6g3p6","tag_id":"ckmm5gsqk000wkzj8cikr2sxe","_id":"ckmm5gsqk000ykzj8f8jp589d"}],"Tag":[{"name":"RNN","_id":"ckmm5gsqe0005kzj8eebxg0a5"},{"name":"BPDC","_id":"ckmm5gsqg000akzj84kf031np"},{"name":"训练","_id":"ckmm5gsqh000dkzj83rz7a1rj"},{"name":"神经网络","_id":"ckmm5gsqh000gkzj82kq4570m"},{"name":"无标度","_id":"ckmm5gsqi000ikzj846y7crf8"},{"name":"网络","_id":"ckmm5gsqi000mkzj858ls0phz"},{"name":"极限环","_id":"ckmm5gsqi000pkzj8fmdm2hdm"},{"name":"matlab","_id":"ckmm5gsqj000skzj84313bnpt"},{"name":"导数","_id":"ckmm5gsqk000tkzj89ptg86sy"},{"name":"中心差分","_id":"ckmm5gsqk000wkzj8cikr2sxe"}]}}